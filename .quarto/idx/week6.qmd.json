{"title":"Clean and ethical data","markdown":{"headingText":"Clean and ethical data","containsRefs":false,"markdown":"\n## Overview\n\nThis week we'll be looking at two topics:\n\n  * preparing less-than-perfect data sets for processing\n  * regulation and ethics of data science\n  \n## Working with clean data\n\nRemember the data science pipeline diagram from Week 1?\n\n![The Data Science Pipeline diagram](Images/DataSciencePipeline.png)\n\nIn this section we'll be focusing on the part labelled _processing code_.\n\nThe processing code carries out some, or all of the following functions:\n\n  1. getting the data\n  2. exploring the structure of the data\n  3. cleaning the data, so it is\n    + technically correct\n    + consistent\n  4. tidying the data\n  \nWe've already looked at Stages 2 and 4. We've also covered some simple methods for Stage 1 - and we will look at more in future weeks. So in this section we'll be concentrating on Stage 3.\n\n#### Technically correct data\n\nWe define technically correct data [@deJongeIDCR] from the point of view of the computer as a dumb machine:\n\n  * will the data actually load, or is it just rejected?\n  * is the data of the right type?\n  * is missing data correctly represented as NAs?\n  * are we using the right encoding for character elements?\n\nHere are some useful rules:\n\n  * create headings with no spaces\n    + use [janitor](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html)\n  * don’t use row names\n    + use [tibble::rownames_to_column](https://tibble.tidyverse.org/reference/rownames.html)\n  * dates in date format\n    + use [lubridate](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) if you need to manipulate dates - but you don't always need to convert (e.g. if you are given a year as an integer, or you are just using the data as a label and the format is fine)\n  * remove columns you don’t need\n  * if you have time series data, consider a time series package like `zoo`\n    + not covered in this module\n  * Strike a balance between doing the minimum necessary to create a dataset that is \"clean\" enough for the job in hand and one that has every single issue fixed so it can be used without further processing for future tasks.\n  * Document what you've done\n\nThe language around quality control isn't always consistent in the way different words are used - but I would call this stage data _verification_.\n\n#### Consistent data\n\nThe next stage is to make sure the data is _consistent_. This means: it is internally logically consistent (e.g. if one column can be calculated from others, it agrees with what you would expect); it is consistent with real-world logic; it is consistent with business logic and assumptions (there may be some overlap between those last two).\n\nProperties of consistent data:\n\n  * it is consistent with the column heading\n  * it has a consistent format\n  * the same thing always has the same label\n  * it is logically consistent\n    + e.g. date_of_birth < date_of_death\n  * it follows “business” rules and assumptions\n    + e.g. a student id consists of eight digits\n    + you may need to get a domain expert to give input here\n    + beware of validation rules that may seem reasonable to you but are actually too restrictive\n  * missing, or invalid data is dealt with appropriately\n\nWith many data sets, the cleaning stage is one of the most time consuming parts of the whole process. In deciding the approach to take, and when the output if good enough, you should think carefully about the requirements of the job and the needs of the end-user. Sometimes you will need 100% correct data and sometimes you won't. You should also be aware of any regulations - the GDPR (see below) has lots to say about data quality.\n\nChecking data for consistency is known as data _validation_ . (Compare with the ability to include data validation rules in Excel.) There is a package called `validate` [@validate] that can help.\n\nDeciding what to do about missing data is often a tricky but crucial decision. Three options are:\n\n  * Remove or ignore records with missing data\n    + be careful when doing this, especially if you decide to remove a whole row (record) because of a single missing column (field)\n  * Try to find another source of information for the missing data\n  * Fill in the missing items with an estimated value (such as the mean, median or a different modelled value).\n  \nThe third option is known as imputation and there is a whole page on CRAN listing R packages in the [Missing Data Task View](https://cran.r-project.org/web/views/MissingData.html).\n\n### A hierarchy for data cleaning\n\n  1. corrections that can be universally applied\n  2. corrections that can be applied to a group identified by a business rule\n  3. ad hoc corrections to single items\n  \nChanges should usually be applied in this order - but remember, it's an iterative process.\n\nAlways make changes programmatically and document your reasons.\n\n## Data regulation\n\nThe UK data protection regime is set out in the Data Protection Act 2018 (DPA) and the General Data Protection Regulation (GDPR) which also forms part of UK law.\n\nThe source for most of this section is the UK [Information Commissioner’s\nOffice]( https://ico.org.uk/ uk/) but the principles apply everywhere.\n\n### What is Data Protection?\n\nData protection is about ensuring people can trust you to use their data fairly and responsibly.\n\nFrom a 2019 survey:\n\nNearly one in three (32%) people have high trust and confidence in companies\nand organisations storing and using their personal information, which is\nslightly down from the 34% stating this in 2018.\n\nThe proportion stating ‘none at all’ has marginally increased from 9% to 10%.\n\nSource: harris interactive [survey](https://ico.org.uk/media/about-the-ico/documents/2615515/ico-trust-and-confidence-report-20190626.pdf) for the Information Commissioner's Office\n\nData protection is the fair and proper use of information about people. It’s part of the fundamental right to privacy but on a more practical level, it’s really about building trust between people and organisations. It’s about treating people fairly and openly, recognising their right to have control over their own identity and their interactions with others, and striking a balance with the wider interests of society.\n\nIt’s also about removing unnecessary barriers to trade and co-operation.\n\nData protection (trust) is essential to innovation.\n\nThe Data Protection Act covers __personal information__, which means information about a particular living individual.\n\nIt doesn’t need to be ‘private’ information: even information which is public knowledge or is about someone’s professional life can be personal data.\n\nIt doesn’t cover truly anonymous information but if you could still identify someone from the details, or by combining it with other information, it will still count as personal data.\n\n### The GDPR sets out seven key principles:\n\n  * Lawfulness, fairness and transparency\n  * Purpose limitation\n  * Data minimisation\n  * Accuracy\n  * Storage limitation\n  * Integrity and confidentiality (security)\n  * Accountability\n  \n#### Lawfulness, fairness and transparency\n\n  * You must identify valid grounds under the GDPR (known as a ‘lawful basis’) for collecting and using personal data.\n  * You must ensure that you do not do anything with the data in breach of any\nother laws.\n  * You must use personal data in a way that is fair. This means you must not\nprocess the data in a way that is unduly detrimental, unexpected or misleading\nto the individuals concerned.\n  * You must be clear, open and honest with people from the start about how you\nwill use their personal data.\n\n#### Purpose limitation\n\n  * You must be clear about what your purposes for processing are from the start.\n  * You need to record your purposes as part of your documentation obligations\nand specify them in your privacy information for individuals.\n  * You can only use the personal data for a new purpose if either this is\ncompatible with your original purpose, you get consent, or you have a clear\nobligation or function set out in law.\n\n#### Data minimisation\n\nYou must ensure the personal data you are processing is:\n\n  * adequate sufficient to properly fulfil your stated purpose;\n  * relevant has a rational link to that purpose; and\n  * limited to what is necessary - you do not hold more than you need for that\npurpose.\n\n#### Accuracy\n\n  * You should take all reasonable steps to ensure the personal data you hold is not incorrect or misleading as to any matter of fact.\n  * You may need to keep the personal data updated, although this will depend on what you are using it for.\n  * If you discover that personal data is incorrect or misleading, you must take reasonable steps to correct or erase it as soon as possible.\n  * You must carefully consider any challenges to the accuracy of personal data.\n  \n#### Storage limitation\n\n  * You must not keep personal data for longer than you need it.\n  * You need to think about and be able to justify how long you keep personal\ndata. This will depend on your purposes for holding the data.\n  * You need a policy setting standard retention periods wherever possible, to\ncomply with documentation requirements.\n  * You should also periodically review the data you hold, and erase or anonymise it when you no longer need it.\n  * You must carefully consider any challenges to your retention of data.\n  * Individuals have a right to erasure if you no longer need the data.\n  * You can keep personal data for longer if you are only keeping it for public interest archiving, scientific or historical research, or statistical purposes.\n  \n#### Integrity and confidentiality (security)\n\n  * You must ensure that you have appropriate security measures in place to\nprotect the personal data you hold.\n\n#### Accountability\n\n  * The accountability principle requires you to take responsibility for what you do with personal data and how you comply with the other principles.\n  * You must have appropriate measures and records in place to be able to\ndemonstrate your compliance.\n\n## Ethical Standards\n\n### The IFoA and RSS Guide for Ethical Data Science\n\nThe Institute and Faculty of Actuaries and the Royal Statistical Society have jointly produced a Guide for Ethical Data Science [@IFoAGEDA] which is quoted below.\n\nThe Guide has five themes:\n\n  * Seek to enhance the value of data science for society\n  * Avoid harm\n  * Apply and maintain professional competence\n  * Seek to preserve or increase trustworthiness\n  * Maintain accountability and oversight\n  \n#### Seek to enhance the value of data science for society{.unnumbered}\n\n>As the impact that data science can have on society could be significant, an important ethical consideration is what the potential implications could be on society as a whole.\n\n>A common theme within ethical frameworks discussing data science and AI is for practitioners to attempt to seek outcomes within their work which support the improvement of public wellbeing. This could involve practitioners seeking to share the benefits of data science and balancing this with the wellbeing of potentially affected individuals.\n\n#### Avoid harm{.unnumbered}\n\n> Data science has the potential to cause harm and this ethical consideration therefore focuses on how practitioners can avoid this by working in a manner that respects the privacy, equality and autonomy of individuals and groups, and speaking up about potential harm or ethical violations.\n\n> Practitioners may be subject to legal and regulatory obligations in relation to the privacy of individuals, relevant to the jurisdiction in which they are working, as well as regulatory obligations to speak up about harm or violations of legal requirements.\n\n> This can also be applied to work relating to businesses, animals or the environment, with consideration of commercial rights, animal welfare and the protection of environmental resources.\n\nThe recent [EU paper on profiling and automatic decision making](https://ec.europa.eu/newsroom/article29/item-detail.cfm?item_id=612053) is of interest in this context.\n\n#### Apply and maintain professional competence{.unnumbered}\n\n> This ethical principle expects data science practitioners to apply best practice and comply with all relevant legal and regulatory requirements, as well as applicable professional body codes.\n\n> Professional competence involves fully understanding the sources of error and bias in data, using ‘clean’ data (eg edited for missing, inconsistent or erroneous values), and supporting work with robust statistical and algorithmic methods that are appropriate to the question being asked.\n\n> Practitioners can also thoroughly assess and balance the benefits of the work versus the risks posed by it, and keep models under regular review.\n\n#### Seek to preserve or increase trustworthiness{.unnumbered}\n\n> The public’s trust and confidence in the work of data scientists can be affected by the way ethical principles are applied. Practitioners can help to increase the trustworthiness of their work by considering ethical principles throughout all stages of a project.\n\n> This is another reoccurring theme that encourages practitioners to be transparent and honest when communicating about the way data is used. Transparency can include fully explaining how algorithms are being used, if and why any decisions have been delegated, and being open about the risks and biases.\n\n> Engaging widely with a diverse range of stakeholders and considering public\nperceptions both from the outset, and throughout projects, can help to build\ntrustworthiness and ensure all potential biases are understood.\n\n#### Maintain accountability and oversight{.unnumbered}\n\n> Another key issue in data ethics around automation and AI is the question of how practitioners maintain human accountability and oversight within their work.\n\n> Being accountable can include being mindful of how and when to delegate any decision making to systems, and having governance in place to ensure systems deliver the intended objectives.\n\n> When deciding to delegate any decision making, it would be useful to fully understand and explain the potential implications of doing so, as the work could lead to introducing advanced AI systems which do not have adequate governance. Practitioners should note that delegating any decisions to these systems does not remove any of their individual responsibilities. \n\n### Other ethics frameworks\n\nYou should research any other ethical frameworks relevant to the jurisdiction and professional area you're working in.\n\nFor example, the UK Government has published a [Data Ethics Framework.](https://www.gov.uk/government/publications/data-ethics-framework)\n\nAnd here is a link to the IEEE's work on the [ethics of autonomous and intelligent systems.](https://standards.ieee.org/industry-connections/ec/autonomous-systems.html)\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"week6.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","number-depth":2,"bibliography":["Z2-book.bib","Z3-packages.bib"],"biblio-style":"apalike","link-citations":"yes","description":"These are the course notes for the 2023 version of Fundamentals of Data Science </br>(MA7419 / MA3419)","theme":{"light":"flatly","dark":"darkly"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"week6.pdf"},"language":{},"metadata":{"block-headings":true,"number-depth":2,"bibliography":["Z2-book.bib","Z3-packages.bib"],"biblio-style":"apalike","link-citations":"yes","description":"These are the course notes for the 2023 version of Fundamentals of Data Science </br>(MA7419 / MA3419)","documentclass":"scrreprt","papersize":"A4"},"extensions":{"book":{}}}}}