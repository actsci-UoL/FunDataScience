[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Data Science",
    "section": "",
    "text": "Congratulations. By the end of this module you will be confident in handling data in a collaborative and reproducible way. In other words in a professional way.\nThis is a vital skill for almost any of the jobs you’re likely to move on to, actuarial or not. The module is for anyone who wants to be a professional (in all the good senses of that word) data scientist.\nThere’s not going to be a lot of listening to lectures. Instead you will be developing the hands-on R programming skills to solve real data problems. And to do that, you’ll need the right software - so that’s the first step. We’ll be using R (R Core Team 2022) and RStudio (Posit team 2023), two amazing - and free - programs.\n\n\n\nIf you have a suitable laptop, you’re probably going to want to install R and RStudio on your own machine. This is good experience and lets you play around absolutely as you want to. Instructions for that are given below and I also strongly suggest you set up the project and folder structure described in the study material for Week 1.\nYou can also run RStudio on University PCs.\nFor most of the work in this module we will be working on a University of Leicester server which you can access from any device that can run a browser. So if you use a Chromebook you will be able to work just as well as people with machines running Windows or Mac OS. You’ll also be able to do a certain amount on a decent sized tablet and you’ll have access via a phone - though I think it would be hard to do any significant work that way.\nOnce your account has been set up (at the beginning of the semester) you should be able to access the server here:\nhttps://rserver.mcs.le.ac.uk/rstudio/\nYou can then log in using your usual University user name (e.g. pk255) and your University password.\nThere is a final option. You could investigate Posit Cloud. Creating a free account there should give you everything you need, again through a browser. The disadvantage is that you only get 15 hours (last time I looked) per month for free. However, it can be useful as a last resort if the other options fail (especially if you’ve kept good backups of your work so you can upload them and carry on seamlessly). If you decide to make an account don’t use your university password on this, or any other cloud accounts.\nWe’ll make sure everyone is up and running in the first week’s sessions, so don’t panic if things don’t work immediately.\nPlease bring your laptop to lectures, that way you’ll be able to follow along with what I’m doing at the front of the room - however, we won’t be able to provide individual support in lectures, that’s what the computer lab sessions are for.\n\n\nThe internet is overflowing with great, free R resources. We’re going to be using a number of them as the main reading for the module, and for instructions on installing software we might as well dive straight into an on-line book we’ll use a lot: R for Data Science (Wickham and Grolemund 2017).\nRead Chapter 1, Introduction (confusingly, Chapter 2 is also called Introduction!). And follow the instructions you’ll find there to install R, RStudio, and the Tidyverse family of packages.\nYou might also want to start reading R Programming for Data Science (Peng 2020). There are instructions for installing the software in Chapter 3 (please ignore Section 3.2).\n\n\n\n\nDataCamp is a website containing a very large number of programming courses - including many on R.\nSubscriptions are usually about 30 USD per month but we have arranged for you to have free access for the duration of this module.\nUsing the link you can find in the Blackboard site you can register for an account and use the resources there to improve your R skills. (You will need your university email address for this one, but do not use your University password.)\nWe’ve found that the DataCamp system can take a while to recognise you’ve signed in with a free account. You might well find that after completing a few free chapters you are invited to sign up for a paid subscription. DO NOT DO THIS. Instead you should just log out and go back in 24 hours, when you should find you can access everything for free.\n\n\n\nThere are a lot of students on this module and just one lecturer and a few TAs. We want you to ask lots of questions, but if you email them all to us we will probably be overwhelmed so please post your questions on the Blackboard forum.\n\n\n\n\nHelp each other.\nBe nice.\nAcknowledge help you get from others. (If someone has been particularly helpful - or nice - let us all know on the forum or email me.)\nDon’t plagiarise, don’t collaborate if you have been asked not to, and don’t perform any other form of academic misconduct. You will almost certainly cut and paste code written by other people - acknowledge its source. The one exception is that you don’t have to acknowledge the source of every bit of code you are given in this course - that could get tedious.\nRespect other people’s right to an undisturbed lecture.\nHelp each other.\n\nQuestion for you: Is this a good list? What should be added or taken away?\n\n\n\n\nPeng, Roger D. 2020. R Programming for Data Science. Morrisville: Lulu.com. https://bookdown.org/rdpeng/rprogdatascience/.\n\n\nPosit team. 2023. RStudio: Integrated Development Environment for r. Boston, MA: Posit Software, PBC. http://www.posit.co/.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  Getting started with R",
    "section": "",
    "text": "On successful completion of this module, you should be able to:\n\nDefine data science and discuss its role in actuarial science and business analytics\nCreate and run simple program scripts in the RStudio environment and read data into R from a local tabular file\nCreate and share literate programs and reports using R Markdown and undertake simple EDA (including summary statistics and informative visualisations) of data in a tabular structure\nImplement a reproducible workflow for simple data science project and describe ethical and regulatory issues to be considered when undertaking a data science project\n\nI prefer an alternative statement of the objectives:\nBy the end of this module you will be confident in handling data in a collaborative and reproducible way. In other words in a professional way. You will be able to:\n\nget\nclean\ncheck\nrestructure\nfilter, sort, join & summarise\nexplore\nvisualise…\n\n…data\nThe specific objectives for this week are:\n\nAccess your R Server account\nUpload Task 1 from Blackboard to the server\nComplete the exercises in the Week 1 task\nKnit to HTML and download HTML and RMD files\nComplete the Week 1 Blackboard test\nSubmit RMD and HTML files on Blackboard\nComplete the Week 1 Reflective Survey\n\n\n\n\nThe structure of this module may be a bit different to what you have come across before.\nIn a “traditional” university lecture you arrive without having done any preparation, the lecturer gives you some information, and you go away to try and absorb it and probably do some exercises or homework.\nIn a “flipped” classroom you are supposed to have done reading, or used other resources, so that you have some familiarity with the material before the class. In class you focus on applying what you have learnt to some exercises.\nThis module is quite similar to a flipped classroom, but here, working on the task and learning what you need to successfully complete it will happen much more in parallel. I will give you a quick overview of some of the things you need to know each week, and there will be some indicative reading. But after that it is up to you (usually working as part of a team) to work out a good solution to the task.\nThis is much more like the way things happen in real life.\n\n\n\n\nDr Paul King FIA (Lecturer)\n\npaul.king@leicester.ac.uk\nNo set office hours: email for an appointment\n\nYour friendly TAs\n\nSilvia Freund - skf10.le.ac.uk\nIbrahim Issahaku - iai5@le.ac.uk\n\n\n\n\n\n\nTuesday 12:00 - 13:00, Rattray Lecture Theatre (please enter through upper doors)\nWednesday 11:00 - 13:00, Computer Labs\n\nCharles Wilson 304/305 for undergraduates and ASDA\nSir Bob Burgess 0.04 for DABI\n\n\n\n\n\n\nA certain amount of lecturing (mostly on Tuesdays)\nA lot of group problem solving (on Wednesday)\nA lot of self study: using the reading list and other on-line resources\nAdditional resources:\n\nData Science Club (if you want to run one!)\nDataCamp\n\nThe weekly tasks for an important part of the learning activity on this module and the completed tasks will be a key part of the set of notes you develop\n\n\n\n\nFrom next week you will be working in groups on a weekly coding task. The output will be a R Markdown notebook containing your code and results - each member of the group must produce their own notebook and be able to edit it and run it on the server or their own machine. Completed tasks (i.e. the PDF file) should be submitted by midnight on Sunday (UK time) in the week of the class.\n\n\n\nYou can ask for help from your TA during the Wednesday computer lab. Outside of the lab (or even inside) you can post questions on the Blackboard discussion forum. Try to formulate your question as specifically as possible and give a code sample. That way the question can be answered in the thread and benefit everybody (other students are encouraged to answer questions). If you need more detailed help a TA will contact you via Teams.\nI’ll create a separate chat for general questions about the course. Please try to put as many of your questions here as possible so everyone can benefit, but if it’s something a bit more personal, please feel free to email me (paul.king@leicester.ac.uk) directly. I’m happy to arrange a one to one meeting over Teams or face to face.\n\n\n\nWhen you’ve completed the weekly task, please submit both your RMD and HTML (sometimes you will be asked for PDF) files via the assignment links on Blackboard.\nYou should also complete the short weekly Blackboard test. This is based on the answers in the exercises, but will sometimes require some extra thinking or research.\nFinally, you should complete the weekly reflective survey. This is designed to act as a prompt for you to reflect on your week’s work. It also gives us a useful indicator of who might need extra help.\nLinks to the test and the survey are in the Blackboard section (the Blackboard term is a Learning Module).\nAll of the above are important “formative” activities to help you learn, give you feedback on your progress, and motivate you to keep up with the work. The School will use them as part of the way it monitors your engagement with the module.\nHowever, none of the marks count toward the final assessment for the module. (Assessments that count for that are called “summative”.)\n\n\n\nThere’s a link to register in the Blackboard site, and there is more information in Getting Set Up.\nIt’s a very good idea to take advantage of this resource while it’s available.\n\n\n\n\nCoursework mid-semester (30%)\n\nsimilar to problems solved in class; but\nyou must work on your own\navailable at the end of week 15 and due in two weeks later\n\nFinal report and presentation (70%)\n\nyou will work on a group data science project and produce both a detailed report of the analysis and a presentation\nthe presentation must include slides (generated from R markdown) but they will be marked on the submitted document - you will not be required to give a stand up presentation\neach individual will have to identify an aspect they worked on, for questions\nsome of the marks will be for demonstrating knowledge of the topics in the lectures after revision week\nassessments for MA3419 and MA7419 are different"
  },
  {
    "objectID": "week1.html#reading",
    "href": "week1.html#reading",
    "title": "1  Getting started with R",
    "section": "1.2 Reading",
    "text": "1.2 Reading\nR for Data Science (Wickham and Grolemund 2017):\n\nChapter 4 Workflow: basics\n\nR Programming for Data Science (Roger D. Peng 2020):\n\nChapter 13 R nuts & bolts\n\nYour own research"
  },
  {
    "objectID": "week1.html#reproducible-data-science",
    "href": "week1.html#reproducible-data-science",
    "title": "1  Getting started with R",
    "section": "1.3 Reproducible data science",
    "text": "1.3 Reproducible data science\n\nIntroduction\nWe are going to discuss Reproducible Data Science. This is often referred to as Reproducible Research, but it applies to lots of data science activities we might not consider as research.\nIt’s helpful to consider the process of data science, for example as illustrated by Roger Peng in his book Report Writing for Data Science in R (Roger D. Peng 2019).\n\n\nThe steps in the shaded box start with the raw (measured) data and end with the computational results. In this module we’ll be concerned with these steps - the core of a reproducible pipeline - plus the presentation code used to produce the final output.\nIn this diagram analytic data refers to the raw data after it has been checked, cleaned and reshaped into the right format for further analysis.\n\n\nWhat is Reproducible Data Science?\nA reproducible data science process allows a reviewer or subsequent user of the work to reproduce the analysis at least from the analytic data to the computational results. It should also allow them to:\n\nsee what has been done at each stage;\nunderstand the reasons for the decisions made at each step;\n\ncheck that the claimed steps have been carried out correctly.\n\nIn all but the simplest of projects, this requires the publication of well written, and well documented, code.\nWhen we consider published work in the context of a scientific publication, reproducibility, as a minimum, requires making available the analytic data, code and details of the computational environment used.\nHowever, it’s important the the whole process, including pre-processing the data and producing the final outputs, is fully documented.\n\n\nBenefits (and disadvantages) of Reproducible Data Science\nScientific:\n\nReplication of published work is a key element of the scientific process: reproduction of the analysis of data analysis is often the closest to replication it’s realistically possible;\nattempts at reproduction can reveal errors with potentially serious consequences.\n\nProfessional:\n\nAbility to reproduce a workflow is key to checking for errors;\nDocumentation of data analysis is required for work review and auditability;\nFor actuaries, the professional standards APS X2 and APS QA2 require consideration to be given to reproducibility in relation to the above points;\nA reproducible work flow allows efficient reuse or modification of an analysis pipeline.\n\nPersonal:\n\nThe next user of your work is most likely to be you!\nthe process of making your work reproducible requires you to stop (or at least slow down) occasionally to make sure you are working effectively and efficiently.\n\n\n\nHow to make sure your work is reproducible\n\nCreate a consistent filing structure for data, code, intermediate and final results;\n\nsee the next section\n\nUse RStudio and knitr to practice literate programming by creating R Markdown notebooks;\nResist the temptation to do things manually - do as much programmatically as possible\n\ncoding all your data manipulation is not just about automating it, it’s about documenting what you’ve done so it can be reproduced;\n\nComment code appropriately;\nStick to a coding standard (style guide)\n\nwe will use the tidyverse style guide\n\nSave as few intermediate results as possible;\nUse version control;\nKeep track of your programming environment\n\nThe last two points are important, but we don’t have the time to go into them in any detail in this course.\n\n\nFurther Reading\nAPS X2 Review of Actuarial Work\nAPS QA1 Quality Assurance for Organisations (Version 2.0)\nThe Institute and Faculty of Actuaries has written about how important Data Science is to the profession."
  },
  {
    "objectID": "week1.html#more-resources",
    "href": "week1.html#more-resources",
    "title": "1  Getting started with R",
    "section": "1.4 More resources",
    "text": "1.4 More resources\nThe Bookdown home page gives you access to many very good books covering different aspects of data science with R."
  },
  {
    "objectID": "week1.html#setting-up-your-r-environment.",
    "href": "week1.html#setting-up-your-r-environment.",
    "title": "1  Getting started with R",
    "section": "1.5 Setting up your R environment.",
    "text": "1.5 Setting up your R environment.\nYou will be working in R every week during this module, so the first task is to make sure you have access to RStudio.\nThe next step is to configure your environment in a standard way so it will be relatively straightforward for us all to work share code and work together.\nGo ahead and:\n\nUse RStudio to create a project called MA3419 or MA7419\nInside that project, create folders for each Week during the Semester, plus a folder called Data\n\nhere is a video demonstrating how to do steps 1 & 2.\n\nInstall the packages ‘tidyverse’ and ‘here’\n\nvideo demonstration\n\nGet familiar with the RStudio interface\n\nvideo\n\nGet familiar with the R Markdown notebook structure and how to run code inside a notebook\n\nvideo\nI highly recommend this blog article\nif you need more help, you’ll almost certainly find something at RStudio\nonce you’re reasonably familiar with RStudio and R Markdown have a look at some more advanced Tips and Tricks (This won’t make much sense if you’re an absolute beginner, but you should bookmark it and come back to it later.)"
  },
  {
    "objectID": "week1.html#week-1-task",
    "href": "week1.html#week-1-task",
    "title": "1  Getting started with R",
    "section": "1.6 Week 1 Task",
    "text": "1.6 Week 1 Task\nEvery week there will be a task provided as a .RMD file in the Blackboard folder. You need to copy this file into the appropriate week’s folder in your MA3419/MA7419 folder and have it open in a running R session during the Wednesday computer lab. The task files will become available on Monday mornings.\n\n\n\n\n\n\nNote\n\n\n\nThe marks for the weekly task are for feedback only. They don’t count towards your module assessment.\n\n\nSometimes there will also be a data file supplied - you should copy this into your project’s Data folder.\nSometimes I’ll work through parts of the task during the Tuesday lecture, and every week I’ll give feedback on the most common mistakes in the previous week’s task. There will also be exercises embedded in the notes for you to complete.\nYour target should be to complete all the exercises in each weekly task, and submit your answers on Blackboard. You should also do the week’s Blackboard test for feedback\nI’ll release the correct code for the previous week’s task on Monday morning, this will include feedback notes based on the submissions where I think that will help.\nOccasionally there will be challenge exercises or challenge tasks - these are optional."
  },
  {
    "objectID": "week1.html#addendum-1-r-markdown-vs-quarto",
    "href": "week1.html#addendum-1-r-markdown-vs-quarto",
    "title": "1  Getting started with R",
    "section": "1.7 Addendum 1: R Markdown vs Quarto",
    "text": "1.7 Addendum 1: R Markdown vs Quarto\nAs you reseach material online you’ll come across references to both R Markdown and Quarto. You’ll also see the option to create Quarto documents (along with a lot of other options) when you start a new file in R Studio.\nR markdown is the original format which is gradually being replaced by Quarto.\nFor our purposes there is very little difference and we’ll stick with R Markdown.\nYou can try it for yourself. Go to File > New File and create two new files: one using R Markdown, and one using Quarto Document.\nLook at the starter code provided (you don’t need to save them) and try to spot the difference.\nYou are welcome to use the Quarto format when you create your own new files, but the weekly tasks are in R Markdown format and we’ll stick with that for now. (Although these notes are actually written in Quarto.)"
  },
  {
    "objectID": "week1.html#addendum-2-r-vs-python",
    "href": "week1.html#addendum-2-r-vs-python",
    "title": "1  Getting started with R",
    "section": "1.8 Addendum 2: R vs Python",
    "text": "1.8 Addendum 2: R vs Python\nPython is an important language for data science and machine learning and I always get asked if you can use Python instead of R.\nThe short answer is “no”, because one of the objectives of this module is for you to learn R.\nHowever, I’m keen to explore the possibility of creating a Python version of this course so if you are interested on working on creating Python code to reproduce the examples and tasks (perhaps as part of a Data Science Club), please email me.\n\n\n\n\nPeng, Roger D. 2020. R Programming for Data Science. Morrisville: Lulu.com. https://bookdown.org/rdpeng/rprogdatascience/.\n\n\nPeng, Roger D. 2019. Report Writing for Data Science in r. British Columbia, Canada: Leanpub. https://leanpub.com/reportwriting.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Tabular data",
    "section": "",
    "text": "This week we’ll be learning about the data structures: vectors, data frames & tibbles.\nThen we’ll be using the dplyr package (part of tidyverse) to begin manipulating tabular data.\nWe’ll be using the dplyr verbs select, filter, mutate and arrange.\nBy the end of this week you’ll be able to:\n\nExplore a tabular data set interactively in the console\nManipulate tabular data using the functions in dplyr\nRead an external CSV or Excel file into R\nFind the documentation for a function or package\nProduce a simple scatter or bar chart in ggplot2\nDraw a simple map with ggplot and add points\nOutput your work as a PDF file from RStudio\n\nBelieve it or not this small set of skills will enable you to do some very interesting exploratory data analysis (EDA)"
  },
  {
    "objectID": "week2.html#reading",
    "href": "week2.html#reading",
    "title": "2  Tabular data",
    "section": "2.2 Reading",
    "text": "2.2 Reading\nR for Data Science (Wickham and Grolemund 2017): - Chapter 5 - Data transformation\nR Programming for Data Science (Peng 2020): - Chapter 5 - Getting Data in and out of R - Chapter 6 - Using the readr package"
  },
  {
    "objectID": "week2.html#digging-deeper-into-the-structure-of-a-data-frame",
    "href": "week2.html#digging-deeper-into-the-structure-of-a-data-frame",
    "title": "2  Tabular data",
    "section": "2.3 Digging deeper into the structure of a data frame",
    "text": "2.3 Digging deeper into the structure of a data frame\nWe’re going to use the starwars dataset that is automatically loaded with the package dplyr. To get it you will have to make sure dplyr is loaded (It’s one of the core tidyverse packages).\nThe first step in most analyses is to explore the data interactively in the console. The most common functions to do this are:\nView(df)\nhead(df)\nsummary(df)\nglimpse(df)\nstr(df)\nYou can watch a walk-through of the use of these functions on the starwars data.\nEach all the elements of each column of a data frame must be of the same type. In starwars there are elements of type numeric, character and list.\nHere is a demonstration of how to extract individual columns from a data frame."
  },
  {
    "objectID": "week2.html#reading-data-from-a-tabular-file",
    "href": "week2.html#reading-data-from-a-tabular-file",
    "title": "2  Tabular data",
    "section": "2.4 Reading data from a tabular file",
    "text": "2.4 Reading data from a tabular file\nThe most common way we’ll be using to get data into R will be to load it from file - usually a CSV or Excel file.\nTo read a CSV file we will use the read_csv function which is part of the readr package loaded with tidyverse (There is a function read.csv which is part of base R, but readr::read_csv is better.)\nThere are lots of optional parameters that you can use to refine the performance of read_csv, but it often works fine with just the path to the file.\nFor example, if we have a file of the 100 most popular girl babies names in England and Wales, in a file called GirlsNames.csv we can import it with the following code.\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\ngirls <- read_csv(here(\"_Data\", \"GirlsNames.csv\"))\nhead(girls, 10)\n\n# A tibble: 10 × 3\n    Rank Name     Count\n   <dbl> <chr>    <dbl>\n 1     1 OLIVIA    3866\n 2     2 AMELIA    3546\n 3     3 ISLA      2830\n 4     4 AVA       2805\n 5     5 MIA       2368\n 6     6 ISABELLA  2297\n 7     7 GRACE     2242\n 8     8 SOPHIA    2236\n 9     9 LILY      2181\n10    10 EMILY     2150\n\n\n(Data downloaded from the ONS)\nQuestions for you: Is this the most up to date list? Can you find a similar list from another country?\nData in Excel files can be read in similarly.\n\nlibrary(readxl)\ncities <- read_excel(here(\"_Data\", \"UK_cities.xlsx\"))\ncities %>% \n  arrange(Latitude) %>% \n  head(10)\n\n# A tibble: 10 × 4\n   City            County      Latitude Longitude\n   <chr>           <chr>          <dbl>     <dbl>\n 1 Truro           Cornwall        50.3    -5.05 \n 2 Plymouth        UK              50.4    -4.14 \n 3 Exeter          the UK          50.7    -3.53 \n 4 Bournemouth     UK              50.7    -1.90 \n 5 Eastbourne      East Sussex     50.8     0.290\n 6 Portsmouth      Hampshire       50.8    -1.09 \n 7 Worthing        West Sussex     50.8    -0.384\n 8 Brighton & Hove East Sussex     50.8    -0.153\n 9 Chichester      West Sussex     50.8    -0.779\n10 Hastings        East Sussex     50.9     0.573"
  },
  {
    "objectID": "week2.html#documentation-and-help",
    "href": "week2.html#documentation-and-help",
    "title": "2  Tabular data",
    "section": "2.5 Documentation and help",
    "text": "2.5 Documentation and help\nYou should get into the habit of looking at the documentation for each function the first time you use it.\nThe first place to look is by using the built-in help in RStudio. You can either go to the help window and use the search, or you can type “?” in the console. For example “?read_csv”. (You’ll usually need to have the relevant package loaded to get help, but “??” might produce something useful.). The help information can sometimes be a bit technical and overwhelming, but there are usually helpful examples at the end.\nHave a look at the help page for read_csv now.\nThe official repository for packages is called CRAN. There you will find the package documentation which always has a PDF reference document with details of all the functions in the package. You’ll also often find one or more vignettes which are tutorial-style documents giving an introduction to the package and maybe more detail on particular aspects.\nTry a web search for “CRAN dplyr” now and see what you can find.\nOf course, there is a lot of other support available online. You can try the tidyverse site or stack overflow"
  },
  {
    "objectID": "week2.html#simple-plots-with-ggplot",
    "href": "week2.html#simple-plots-with-ggplot",
    "title": "2  Tabular data",
    "section": "2.6 Simple plots with ggplot",
    "text": "2.6 Simple plots with ggplot\nNow we know how to get data into R it won’t be long before you want to plot it.\nThere are a number of alternatives but we’ll be using ggplot. The syntax can take a bit of getting used to, so here are a couple of simple examples.\nThere are three different components to making a plot in ggplot.\n\na data frame containing the data you want to plot\nthe type of plot you want\nthe columns containing the data to be plotted\n\nHere’s the simplest possible example using the starwars data. (here’s a walk-through)\n\nlibrary(ggplot2)\nplot_data <- \n  starwars %>% \n  filter(mass < 1000) # this is explained in the walk-through\n\nggplot(plot_data) + \n  geom_point(aes(x = height, y = mass, color = sex))\n\n\n\n\nPart 1 is dealt with by passing the data frame as a parameter to ggplot.\nPart 2 is dealt with by choosing an appropriate geom_ function.\nPart 3 is dealt with by the parameters to the aes function. Note that different geoms have different aes requirements. See the built in help for the particular geom.\nHere’s another example:\n\nggplot(plot_data) + \n  geom_bar(aes(x = sex), fill = \"skyblue\")\n\n\n\n\nFor a quick and easy guide to producing many types of plots see the R Graphics Cookbook (Chang 2020)."
  },
  {
    "objectID": "week2.html#question-for-you",
    "href": "week2.html#question-for-you",
    "title": "2  Tabular data",
    "section": "Question for you",
    "text": "Question for you\n\n\n\n\n\n\nHow can you add a title to a chart and change the axis labels?\n\n\n\n\n\nUse ggtitle(), xlab() and ylab().\n\nggplot(plot_data) + \n  geom_bar(aes(x = sex), fill = \"skyblue\") +\n  ggtitle(\"Number of Starwars characters by sex\") +\n  xlab(\"Sex\") +\n  ylab(\"Number\")"
  },
  {
    "objectID": "week2.html#simple-maps-with-ggplot",
    "href": "week2.html#simple-maps-with-ggplot",
    "title": "2  Tabular data",
    "section": "2.7 Simple maps with ggplot",
    "text": "2.7 Simple maps with ggplot\nThere’s no denying that manipulating and visualizing spatial data can be very complex and most of the techniques are far beyond the scope of this course. But it’s such a powerful visualization method that I thought it was important to give you a way to get started by plotting points on a simple map.\nIf you want to find out more about this topic I recommend Geocomputation with R (Robin Lovelace 2020)\nThere are many formats for storing spatial data. In this example we use two: shapefiles and the relatively new Simple Features,\nShapefiles are well established, and most publishers of geographical information will make it available in this format (possibly alongside others)\nThe simple features format allows us to work with geographical data in the familiar form of a data frame, and to plot maps using ggplot. We’ll need to install the sf package before we can get started.\nWe are going to use low resolution country boundary shapefiles from here. (I’ve already downloaded files for the UK and Ireland and put them in Blackboard.)\nThe first bit of code reads in two files and converts them to sf objects.\n\nlibrary(sf)\nIreland <- \n  st_read(here(\"_Data\", \"Ireland_Boundaries-shp\", \"Country_Boundaries.shp\"),\n          quiet = TRUE)\nUK <- \n  st_read(here(\"_Data\", \"UK_Boundaries-shp\", \"Country_Boundaries.shp\"),\n          quiet = TRUE)\n\nLet’s look at what’s inside one of these sf objects.\n\nglimpse(Ireland)\n\nRows: 1\nColumns: 8\n$ OBJECTID_1 <int> 104\n$ OBJECTID   <int> 104\n$ name       <chr> \"Ireland\"\n$ Id         <int> 0\n$ Shape_Leng <dbl> 71.55104\n$ Shape_Le_1 <dbl> 71.55104\n$ Shape_Area <dbl> 9.443232\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-9.46639 51...\n\n\nWe can see there is only one row and 8 columns. The row contains attributes and geometry information about a single feature - crucially it contains geometry information which, in this case defines a set of polygons which make up the boundary of Ireland.\nSince they came from the same source, the UK file contains the same columns, so we can stick them together to create a single object which we can then plot using the sf_geom provided by ggplot2\n\nUK_IRL <- bind_rows(UK, Ireland)\nm <- ggplot() + geom_sf(data = UK_IRL) # We can plot m now, and add to it later.\nm\n\n\n\n\nTo demonstrate how we can add additional points to this data, we’ll add the cities we listed above to the map. We’ll also change the limits of the plot to zoom in on the South East of the country.\n\nm + geom_point(data = cities, aes(x = Longitude, y = Latitude)) +\n  xlim(c(-2, 2)) +\n  ylim(c(50, 52))\n\n\n\n\nAt this scale we can see the low resolution of the boundary information."
  },
  {
    "objectID": "week2.html#week-2-task",
    "href": "week2.html#week-2-task",
    "title": "2  Tabular data",
    "section": "2.8 Week 2 Task",
    "text": "2.8 Week 2 Task\nThe task for Week 2 can be found in the Week 2 folder on Blackboard. This week you should knit your file to PDF and upload the RMD and PDF files.\n\n\n\n\nChang, Winston. 2020. R Graphics Cookbook. O’Reilly Media. https://r-graphics.org/.\n\n\nPeng, Roger D. 2020. R Programming for Data Science. Morrisville: Lulu.com. https://bookdown.org/rdpeng/rprogdatascience/.\n\n\nRobin Lovelace, Jannes Muenchow, Jakub Nowosad. 2020. Geocomputation with r. CRC Press. https://geocompr.robinlovelace.net/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Group and summarise",
    "section": "",
    "text": "This week we’ll be doing more manipulation of tabular data using the dplyr(Wickham et al. 2023) verbs group() and summarise().\nWe’ll also be creating more bar charts using ggplot2(Wickham 2016).\nBy the end of this week you’ll be able to:\n\nGroup and summarise data in a tabular data set\nUse table() and functions from the janitor package to create cross-tab summaries\nUse functions from the forcats package to work with factors\n\nYou’ll need to use the references and the R help system to develop the above skills as you work through the Week 3 task."
  },
  {
    "objectID": "week3.html#reading",
    "href": "week3.html#reading",
    "title": "3  Group and summarise",
    "section": "3.2 Reading",
    "text": "3.2 Reading\nR for Data Science (Wickham and Grolemund 2017):\n\nChapter 3 Data visualisation\nChapter 5 Data transformation\nChapter 6 Workflow: scripts\nChapter 7 Exploratory Data Analysis\nChapter 8 Workflow: projects\n\nR Programming for Data Science (Peng 2020):\n\nChapter 12 Managing Data Frames with the dplyr package"
  },
  {
    "objectID": "week3.html#making-slides-in-r-markdown",
    "href": "week3.html#making-slides-in-r-markdown",
    "title": "3  Group and summarise",
    "section": "3.3 Making slides in R Markdown",
    "text": "3.3 Making slides in R Markdown\nYou’ll need to make slides for the final assessment - so here’s a preview. It’s very easy - you can do it straight from the menu:\nFile > New File > R Markdown…\nSee the guide to producing slides (part of a larger introduction to using R Markdown from the makers of RStudio.)"
  },
  {
    "objectID": "week3.html#debugging",
    "href": "week3.html#debugging",
    "title": "3  Group and summarise",
    "section": "3.4 Debugging",
    "text": "3.4 Debugging\n\nCode almost never works properly the first time you write it\n\n\nRunning into bugs and errors is frustrating, but it’s also an opportunity to learn a bit more.\n\n\nErrors can be obscure, but they are usually not malicious or random.\n\n\nIf something has gone wrong, you can find out why it happened."
  },
  {
    "objectID": "week3.html#knitting-to-html-hints",
    "href": "week3.html#knitting-to-html-hints",
    "title": "3  Group and summarise",
    "section": "3.5 Knitting to HTML (hints)",
    "text": "3.5 Knitting to HTML (hints)\n\nmake sure all your packages are up to date.\ndoes all your .RMD code run properly (i.e. if you click “run all” does it run without any error messages) - if it doesn’t, fix it.\nif (2) works, start a new R session on the server (for example, by closing down and restarting) - does the RMD code still run? If not, fix it.\nonce the RMD file runs, try clicking on knit to HTML, again, if that fails and you can’t fix it, ask for help.\nIf you can knit to HTMl, you can try knitting to Word and PDF.\nIf you can knit to Word, you can create a PDF file by exporting from Word\nIf you can’t create a PDF file in any other way you can try this as a last resort. If knitting to HTML works and opens an HTML file in a viewer window, click on “view in browser”. If you are using Chrome, you will be able to save as a PDF document under “print”. I advise installing Chrome for UoL work because that is the standard on University machines so most things are tested on it."
  },
  {
    "objectID": "week3.html#week-3-task",
    "href": "week3.html#week-3-task",
    "title": "3  Group and summarise",
    "section": "3.6 Week 3 Task",
    "text": "3.6 Week 3 Task\nSome bigger files to work on this week, and some wider research required.\nThe task is on Blackboard and on the server at\n/data/FDS/Scripts/\n/data/FDS/Data/"
  },
  {
    "objectID": "week3.html#check-your-understanding",
    "href": "week3.html#check-your-understanding",
    "title": "3  Group and summarise",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\n\n\n\n\nWhat is a “vignette” in the world of R?\n\n\n\n\n\nA vignette is a readble introduction to a package, or a particular aspect of it. It is typically more friendly than the bare package documentation (such as the reference PDF file found on CRAN) and contains code examples in a tutorial style.\n\n\n\n\n\n\n\n\n\nFind the vignette called Introduction to dplyr and study it.\n\n\n\n\n\nHere’s the link to Introduction to dplyr.\n\n\n\n\n\n\n\nPeng, Roger D. 2020. R Programming for Data Science. Morrisville: Lulu.com. https://bookdown.org/rdpeng/rprogdatascience/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "4  Working with tidy data",
    "section": "",
    "text": "This week we’ll be examining the concept of tidy data and looking at how to pivot between different data formats.\nWe’ll also be looking at an important method of searching and manipulating strings called regular expressions.\nBy the end of this week you’ll be able to:\n\nDefine the concept of tidy data and describe when it’s useful\nConvert between narrower (tidy) data frames and wider ones using the functions in tidyr\nDescribe the purpose and structure of a regular expression\nUse regular expressions in conjunction with the stringr package\n\nAs a special bonus, you’ll be able to use regular expressions for searching inside Microsoft Office documents."
  },
  {
    "objectID": "week4.html#tidy-data",
    "href": "week4.html#tidy-data",
    "title": "4  Working with tidy data",
    "section": "4.2 Tidy data",
    "text": "4.2 Tidy data\nTidy data is data where:\n\nEvery column is a variable.\nEvery row is an observation..\nEvery cell is a single value.\n\nTidy data describes a standard way of storing data that is used wherever possible throughout the tidyverse. If you ensure that your data is tidy, you’ll spend less time fighting with the tools and more time working on your analysis.\nHowever, tidy data isn’t always the best solution. In particular humans usually find wider data frames easier to take in and understand, so for presentation and data entry tasks the wider format is often best.\nHere’s an example of a small tidy data set:\n\n\n# A tibble: 6 × 3\n  Name     Test   Score\n  <chr>   <int>   <dbl>\n1 Tiddles     1 -0.470 \n2 Rover       1 -0.161 \n3 Tiddles     2  0.0958\n4 Rover       2 -2.14  \n5 Tiddles     3  2.49  \n6 Rover       3  0.553 \n\n\nAnd here’s the same data in a non-tidy (wide) format.\n\nwide_df <- \n  tidy_df |> \n    pivot_wider(names_from = Test, \n                names_prefix = \"Test_\", \n                values_from = Score)\n\nwide_df\n\n# A tibble: 2 × 4\n  Name    Test_1  Test_2 Test_3\n  <chr>    <dbl>   <dbl>  <dbl>\n1 Tiddles -0.470  0.0958  2.49 \n2 Rover   -0.161 -2.14    0.553\n\n\nThe functions in the tidyverse (notably ggplot) expect data in a tidy format but the wider format is often easier for people to read or to use when entering data.\nAs you’ve just seen, the pivot_wider() function takes us from tidy (aka “longer”) format to a wider format. We can go back with pivot_longer(). Notice that the parameters are matched to those in the ‘pivot wider()’ example above.\n\nwide_df |> \n  pivot_longer(cols = !Name,              # Apply to all columns except Name\n               names_to = \"Test\",\n               names_prefix = \"Test_\",    # Remove this from the names.\n               values_to = \"Score\",\n               ) \n\n# A tibble: 6 × 3\n  Name    Test    Score\n  <chr>   <chr>   <dbl>\n1 Tiddles 1     -0.470 \n2 Tiddles 2      0.0958\n3 Tiddles 3      2.49  \n4 Rover   1     -0.161 \n5 Rover   2     -2.14  \n6 Rover   3      0.553 \n\n\nFor more details, see the pivot vignette.\nNote: pivot_wider() and pivot_longer() replaced the previous functions spread() and gather(), which you may still see around."
  },
  {
    "objectID": "week4.html#regular-expressions",
    "href": "week4.html#regular-expressions",
    "title": "4  Working with tidy data",
    "section": "4.3 Regular expressions",
    "text": "4.3 Regular expressions\nRegular expressions can be thought of as a mechanism for advanced search. They provide a language for writing a pattern which is used to match occurrences withing a character string. This ability is made use of by functions in many different languages - in R we’ll mainly use them in conjunction with the ‘stringr’ package.\nHere’s a simple example. Suppose the string we want to search is “Paul wrote this in 2020.”\nThe simplest search would be to see if target contains the string “Paul”, like this.\n\nlibrary(stringr)\ntarget <- \"Paul wrote this in 2020.\"\ntarget |> \n  str_detect(pattern = \"Paul\")\n\n[1] TRUE\n\n\nOr we could extract the year contained in the target using a pattern that finds any string of exactly 4 digits.\n\ntarget |> \n  str_extract(pattern = \"\\\\d{4}\")\n\n[1] \"2020\"\n\n\nYou’ll see that a regular expression (or regex) will often contain a special character like \\\\d, which matches any digit. The double backslash is a quirk of R. Most implementations of regular expressions only require a single backslash, so if you use any resource not written specifically for R you’ll see \\d rather than \\\\d.\nIn fact (rather like SQL) there are minor differences in the implementation across different languages: something to bear in mind if you get stuck debugging some regex code.\nOur treatment in the lecture is by no means comprehensive - regular expressions can be considered a language in their own right. See:\nRegular expressions\nMy favourite quick reference is:\nRegex Cheat Sheet\nFor stringr see:\nIntroduction to stringr\nThe university library has a book called Mastering Regular Expressions (Friedl 2006)."
  },
  {
    "objectID": "week4.html#reading",
    "href": "week4.html#reading",
    "title": "4  Working with tidy data",
    "section": "4.4 Reading",
    "text": "4.4 Reading\nR for Data Science (Wickham and Grolemund 2017):\n\nChapter 12 Tidy data\n\nR Programming for Data Science (Peng 2020):\n\nChapter 13 Control structures\nChapter 14 Functions\nChapter 17 Regular expressions"
  },
  {
    "objectID": "week4.html#check-your-understanding",
    "href": "week4.html#check-your-understanding",
    "title": "4  Working with tidy data",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\n\n\n\n\nIn the regular expression “\\\\d{4}”, why are there two backslashes?\n\n\n\n\n\nThis is a peculiarity of using regular expressions in R. The first backslash tells R to treat the second backslash as a ‘backslash’ and not a special character.\n\n\n\n\n\n\n\n\n\nWhat pattern would “\\\\D{4}” match?\n\n\n\n\n\nThis would match any four characters that are NOT digits. (See also w vs W - i.e. lower-case w vs upper-case W).\n\n\n\n\n\n\n\n\n\nIn the world of regular expressions, what does “greedy” mean?\n\n\n\n\n\nA greedy expression matches the longest possible pattern it can find in the target. A lazy expression takes the first matching pattern it comes across (i.e. the shortest).\nFor example, here we try extracting 2 or more digits after the decimal point:\n\n# Greedy...\nstr_extract(\"The value of pi is 3.14159\", \"\\\\d\\\\.\\\\d{2,}\")\n\n[1] \"3.14159\"\n\n# Lazy (note the question mark)...\nstr_extract(\"The value of pi is 3.14159\", \"\\\\d\\\\.\\\\d{2,}?\")\n\n[1] \"3.14\"\n\n\n\n\n\n\n\n\n\nFriedl, Jeffrey E. F. 2006. Mastering Regular Expressions. 3rd ed.. Sebastapol, Calif.: O’Reilly.\n\n\nPeng, Roger D. 2020. R Programming for Data Science. Morrisville: Lulu.com. https://bookdown.org/rdpeng/rprogdatascience/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "week5.html#overview",
    "href": "week5.html#overview",
    "title": "5  Joining tables",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nThis week we’ll be looking at how to combine different tables of data.\nBy the end of this chapter you’ll be able to:\n\nJoint data frames using dplyr::bind_rows() or dplyr::bind_cols()\nDescribe and use the following dplyr mutating joins:\n\n\nleft_join()\nright_join()\ninner_join()\nfull_join()\n\n\nDescribe and use the following dplyr filtering joins:\n\n\nsemi_join()\nanti_join()"
  },
  {
    "objectID": "week5.html#joining-tables",
    "href": "week5.html#joining-tables",
    "title": "5  Joining tables",
    "section": "5.2 Joining Tables",
    "text": "5.2 Joining Tables\nIt’s a common problem to have to combine data contained in two different tables. (We’ll always assume in R that we are referring to data frames when we talk about a table - although there are other table-like structures)\n\nBinding rows\nOne simple case is when we have two tables with identical columns and we want to “stick” one onto the bottom of the other. In Excel this is often done by cutting and pasting, which is a very dangerous method since it’s so easy to lose rows.\nIn R we do this using dplyr::bind_rows(). (This is the tidyverse equivalent of rbind().)\n\nlibrary(dplyr)\nlibrary(stringr)\n# Split up mtcars\nall_cars <- mtcars |> arrange(wt)\nbig_cars <- filter(all_cars, wt > 3.5 )\nlittle_cars <- filter(all_cars, wt <= 3.5)\n\n# Recombine\ncombined_rows <- bind_rows(big_cars, little_cars) |> arrange(wt)\n\n# Test for equality\nidentical(all_cars, combined_rows)\n\n[1] TRUE\n\n\nA few things to note:\n\nIf one of the data frames has a column that isn’t in the other, it will be filled with NAs in the output;\nThe columns don’t have to be in the same order, they just have to have the same names;\nIf there are are duplicated rows in the inputs, they will be duplicated in the output (see union() for a way to avoid this if both data frames have identical columns);\nbind_rows() can take any number of inputs - contained in a list;\nIf you are trying to join many data frames (inside a loop for example) it is much faster to collect them in a list and then bind them all at once, than it is to bind each one inside the loop.\nYou can use the .id parameter to record the source of each row (see ?bind_rows).\nThe set functions union(), intersect() and setdif() can also be useful.\n\n\n\nBinding columns\nIf we have the same number of rows in two data frames we can add the columns using bind_cols().\n\n# Split up mtcars\ncars1 <- all_cars |> select(1:4)\ncars2 <- all_cars |> select(-(1:4))\n\n#Recombine\ncombined_cols <- bind_cols(cars1, cars2)\n\n# Test for equality\nidentical(all_cars, combined_cols)\n\n[1] TRUE"
  },
  {
    "objectID": "week5.html#mutating-joins",
    "href": "week5.html#mutating-joins",
    "title": "5  Joining tables",
    "section": "5.3 Mutating Joins",
    "text": "5.3 Mutating Joins\nThe following joins allow us to combine the data from two tables, creating extra columns as necessary.\nA nice feature of these `join_ functions is that their names and behaviour are similar to analogous functions for joining data in SQL, as we will see later in the programme.\nSuppose I have a table containing some details of students registered on a particular module. Perhaps it contains the student ID and a mark for a particular assignment.\n\nset.seed(123)\nmarks <- \n  tibble(ID = 1:5,\n         Score = round(rnorm(5, mean = 65, sd = 10), 1))\n\nmarks$ID[5] <- 25\nmarks\n\n# A tibble: 5 × 2\n     ID Score\n  <dbl> <dbl>\n1     1  59.4\n2     2  62.7\n3     3  80.6\n4     4  65.7\n5    25  66.3\n\n\nI also have a table listing students and whether they are UG or PGT and their email address.\n\ndetails <- \n  tibble( ID = 1:10,\n          Level = sample(c(\"UG\", \"PGT\"), size = 10, replace = TRUE)) |> \n  mutate(Email = str_c(\"stu\", ID, \"@univ.ac.uk\"))\n\nis.na(details$Email) <- 3\n\ndetails\n\n# A tibble: 10 × 3\n      ID Level Email           \n   <int> <chr> <chr>           \n 1     1 PGT   stu1@univ.ac.uk \n 2     2 PGT   stu2@univ.ac.uk \n 3     3 PGT   <NA>            \n 4     4 UG    stu4@univ.ac.uk \n 5     5 PGT   stu5@univ.ac.uk \n 6     6 UG    stu6@univ.ac.uk \n 7     7 PGT   stu7@univ.ac.uk \n 8     8 UG    stu8@univ.ac.uk \n 9     9 UG    stu9@univ.ac.uk \n10    10 UG    stu10@univ.ac.uk\n\n\nNow suppose I want to add the email address to the first data frame. In Excel we could use something like vlookup or index/match. In R we can use a left_join() which takes each row in the second table and adds the columns from the second if it finds a match. In my experience this is the most commonly used join.\n\nmarks |> \n  left_join(details, by = \"ID\")\n\n# A tibble: 5 × 4\n     ID Score Level Email          \n  <dbl> <dbl> <chr> <chr>          \n1     1  59.4 PGT   stu1@univ.ac.uk\n2     2  62.7 PGT   stu2@univ.ac.uk\n3     3  80.6 PGT   <NA>           \n4     4  65.7 UG    stu4@univ.ac.uk\n5    25  66.3 <NA>  <NA>           \n\n\nIf you don’t want the Level column it can be filtered out after the join.\nright_join() includes all the rows in the second table.\n\nmarks |> \n  right_join(details, by = \"ID\")\n\n# A tibble: 10 × 4\n      ID Score Level Email           \n   <dbl> <dbl> <chr> <chr>           \n 1     1  59.4 PGT   stu1@univ.ac.uk \n 2     2  62.7 PGT   stu2@univ.ac.uk \n 3     3  80.6 PGT   <NA>            \n 4     4  65.7 UG    stu4@univ.ac.uk \n 5     5  NA   PGT   stu5@univ.ac.uk \n 6     6  NA   UG    stu6@univ.ac.uk \n 7     7  NA   PGT   stu7@univ.ac.uk \n 8     8  NA   UG    stu8@univ.ac.uk \n 9     9  NA   UG    stu9@univ.ac.uk \n10    10  NA   UG    stu10@univ.ac.uk\n\n\nIn this case we end up with 10 rows (as in the details table.)\ninner_join() only matches rows that occur in both tables.\n\nmarks |> \n  inner_join(details, by = \"ID\")\n\n# A tibble: 4 × 4\n     ID Score Level Email          \n  <dbl> <dbl> <chr> <chr>          \n1     1  59.4 PGT   stu1@univ.ac.uk\n2     2  62.7 PGT   stu2@univ.ac.uk\n3     3  80.6 PGT   <NA>           \n4     4  65.7 UG    stu4@univ.ac.uk\n\n\nFinally, full_join() includes all rows in marks OR details.\n\nmarks |> \n  full_join(details, by = \"ID\")\n\n# A tibble: 11 × 4\n      ID Score Level Email           \n   <dbl> <dbl> <chr> <chr>           \n 1     1  59.4 PGT   stu1@univ.ac.uk \n 2     2  62.7 PGT   stu2@univ.ac.uk \n 3     3  80.6 PGT   <NA>            \n 4     4  65.7 UG    stu4@univ.ac.uk \n 5    25  66.3 <NA>  <NA>            \n 6     5  NA   PGT   stu5@univ.ac.uk \n 7     6  NA   UG    stu6@univ.ac.uk \n 8     7  NA   PGT   stu7@univ.ac.uk \n 9     8  NA   UG    stu8@univ.ac.uk \n10     9  NA   UG    stu9@univ.ac.uk \n11    10  NA   UG    stu10@univ.ac.uk\n\n\nNote that if the second table (details) contains more than one row that matches with ID in marks there will be more than one corresponding row in the output.\n\ndetails <- \n  details |> \n  bind_rows(tibble(ID = 4, Level = \"PGT\", Email = \"stu4@univ.ac.uk\"))\n\n\nmarks |> \n  left_join(details, by = \"ID\")\n\n# A tibble: 6 × 4\n     ID Score Level Email          \n  <dbl> <dbl> <chr> <chr>          \n1     1  59.4 PGT   stu1@univ.ac.uk\n2     2  62.7 PGT   stu2@univ.ac.uk\n3     3  80.6 PGT   <NA>           \n4     4  65.7 UG    stu4@univ.ac.uk\n5     4  65.7 PGT   stu4@univ.ac.uk\n6    25  66.3 <NA>  <NA>           \n\n\n\nFiltering joins\nSometimes we want to remove rows from the first table, dependent on the contents of the second. For example we can use the information about student level in details to pick out any students with out a matching entry.\n\nmarks |> \n  anti_join(details, by = \"ID\")\n\n# A tibble: 1 × 2\n     ID Score\n  <dbl> <dbl>\n1    25  66.3\n\n\nOr we can use a list of UG students (in this case generated by filtering details) to pick out the UG students in marks.\n\nmarks |> \n  semi_join(details |> filter(Level ==\"UG\"), by = \"ID\")\n\n# A tibble: 1 × 2\n     ID Score\n  <dbl> <dbl>\n1     4  65.7"
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "5  Joining tables",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nMutating joins add columns from y to x, matching rows based on the keys passed with the by = parameter:\ninner_join() includes all rows in x and y.\nleft_join() includes all rows in x.\nright_join() includes all rows in y.\nfull_join() includes all rows in x or y.\nIf a row in x matches multiple rows in y, all the rows in y will be returned once for each matching row in x.\nFiltering joins filter rows from x based on the presence or absence of matches in y:\nsemi_join() returns all rows from x with a match in y.\nanti_join() returns all rows from x without a match in y."
  },
  {
    "objectID": "week5.html#reading",
    "href": "week5.html#reading",
    "title": "5  Joining tables",
    "section": "5.5 Reading",
    "text": "5.5 Reading\nR for Data Science (Wickham and Grolemund 2017):\n\nChapter 13 Relational data"
  },
  {
    "objectID": "week5.html#check-your-understanding",
    "href": "week5.html#check-your-understanding",
    "title": "5  Joining tables",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\n\n\n\n\nWhat would you expect the output from full_join(big_cars, little_cars) to be?\n\n\n\n\n\n\nfull_join(big_cars, little_cars)\n\nJoining with `by = join_by(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear,\ncarb)`\n\n\n    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n1  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n3  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n6  13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n7  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n8  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n9  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n10 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n11 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n12 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n13 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n14 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n15 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n16 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n17 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n18 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n19 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n20 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n21 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n22 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n23 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n24 22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n25 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n26 24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n27 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n28 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n29 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n30 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n31 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n32 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "6  Clean and ethical data",
    "section": "",
    "text": "This week we’ll be looking at two topics:\n\npreparing less-than-perfect data sets for processing\nregulation and ethics of data science"
  },
  {
    "objectID": "week6.html#working-with-clean-data",
    "href": "week6.html#working-with-clean-data",
    "title": "6  Clean and ethical data",
    "section": "6.2 Working with clean data",
    "text": "6.2 Working with clean data\nRemember the data science pipeline diagram from Week 1?\n\n\n\nThe Data Science Pipeline diagram\n\n\nIn this section we’ll be focusing on the part labelled processing code.\nThe processing code carries out some, or all of the following functions:\n\ngetting the data\nexploring the structure of the data\ncleaning the data, so it is + technically correct + consistent\ntidying the data\n\nWe’ve already looked at Stages 2 and 4. We’ve also covered some simple methods for Stage 1 - and we will look at more in future weeks. So in this section we’ll be concentrating on Stage 3.\n\nTechnically correct data\nWe define technically correct data (Jonge and Loo 2013) from the point of view of the computer as a dumb machine:\n\nwill the data actually load, or is it just rejected?\nis the data of the right type?\nis missing data correctly represented as NAs?\nare we using the right encoding for character elements?\n\nHere are some useful rules:\n\ncreate headings with no spaces\n\nuse janitor\n\ndon’t use row names\n\nuse tibble::rownames_to_column\n\ndates in date format\n\nuse lubridate if you need to manipulate dates - but you don’t always need to convert (e.g. if you are given a year as an integer, or you are just using the data as a label and the format is fine)\n\nremove columns you don’t need\nif you have time series data, consider a time series package like zoo\n\nnot covered in this module\n\nStrike a balance between doing the minimum necessary to create a dataset that is “clean” enough for the job in hand and one that has every single issue fixed so it can be used without further processing for future tasks.\nDocument what you’ve done\n\nThe language around quality control isn’t always consistent in the way different words are used - but I would call this stage data verification.\n\n\nConsistent data\nThe next stage is to make sure the data is consistent. This means: it is internally logically consistent (e.g. if one column can be calculated from others, it agrees with what you would expect); it is consistent with real-world logic; it is consistent with business logic and assumptions (there may be some overlap between those last two).\nProperties of consistent data:\n\nit is consistent with the column heading\nit has a consistent format\nthe same thing always has the same label\nit is logically consistent\n\ne.g. date_of_birth < date_of_death\n\nit follows “business” rules and assumptions\n\ne.g. a student id consists of eight digits\nyou may need to get a domain expert to give input here\nbeware of validation rules that may seem reasonable to you but are actually too restrictive\n\nmissing, or invalid data is dealt with appropriately\n\nWith many data sets, the cleaning stage is one of the most time consuming parts of the whole process. In deciding the approach to take, and when the output if good enough, you should think carefully about the requirements of the job and the needs of the end-user. Sometimes you will need 100% correct data and sometimes you won’t. You should also be aware of any regulations - the GDPR (see below) has lots to say about data quality.\nChecking data for consistency is known as data validation . (Compare with the ability to include data validation rules in Excel.) There is a package called validate (Loo and Jonge 2021) that can help.\nDeciding what to do about missing data is often a tricky but crucial decision. Three options are:\n\nRemove or ignore records with missing data\n\nbe careful when doing this, especially if you decide to remove a whole row (record) because of a single missing column (field)\n\nTry to find another source of information for the missing data\nFill in the missing items with an estimated value (such as the mean, median or a different modelled value).\n\nThe third option is known as imputation and there is a whole page on CRAN listing R packages in the Missing Data Task View.\n\n\nA hierarchy for data cleaning\n\ncorrections that can be universally applied\ncorrections that can be applied to a group identified by a business rule\nad hoc corrections to single items\n\nChanges should usually be applied in this order - but remember, it’s an iterative process.\nAlways make changes programmatically and document your reasons."
  },
  {
    "objectID": "week6.html#data-regulation",
    "href": "week6.html#data-regulation",
    "title": "6  Clean and ethical data",
    "section": "6.3 Data regulation",
    "text": "6.3 Data regulation\nThe UK data protection regime is set out in the Data Protection Act 2018 (DPA) and the General Data Protection Regulation (GDPR) which also forms part of UK law.\nThe source for most of this section is the UK Information Commissioner’s Office but the principles apply everywhere.\n\nWhat is Data Protection?\nData protection is about ensuring people can trust you to use their data fairly and responsibly.\nFrom a 2019 survey:\nNearly one in three (32%) people have high trust and confidence in companies and organisations storing and using their personal information, which is slightly down from the 34% stating this in 2018.\nThe proportion stating ‘none at all’ has marginally increased from 9% to 10%.\nSource: harris interactive survey for the Information Commissioner’s Office\nData protection is the fair and proper use of information about people. It’s part of the fundamental right to privacy but on a more practical level, it’s really about building trust between people and organisations. It’s about treating people fairly and openly, recognising their right to have control over their own identity and their interactions with others, and striking a balance with the wider interests of society.\nIt’s also about removing unnecessary barriers to trade and co-operation.\nData protection (trust) is essential to innovation.\nThe Data Protection Act covers personal information, which means information about a particular living individual.\nIt doesn’t need to be ‘private’ information: even information which is public knowledge or is about someone’s professional life can be personal data.\nIt doesn’t cover truly anonymous information but if you could still identify someone from the details, or by combining it with other information, it will still count as personal data.\n\n\nThe GDPR sets out seven key principles:\n\nLawfulness, fairness and transparency\nPurpose limitation\nData minimisation\nAccuracy\nStorage limitation\nIntegrity and confidentiality (security)\nAccountability\n\n\nLawfulness, fairness and transparency\n\nYou must identify valid grounds under the GDPR (known as a ‘lawful basis’) for collecting and using personal data.\nYou must ensure that you do not do anything with the data in breach of any other laws.\nYou must use personal data in a way that is fair. This means you must not process the data in a way that is unduly detrimental, unexpected or misleading to the individuals concerned.\nYou must be clear, open and honest with people from the start about how you will use their personal data.\n\n\n\nPurpose limitation\n\nYou must be clear about what your purposes for processing are from the start.\nYou need to record your purposes as part of your documentation obligations and specify them in your privacy information for individuals.\nYou can only use the personal data for a new purpose if either this is compatible with your original purpose, you get consent, or you have a clear obligation or function set out in law.\n\n\n\nData minimisation\nYou must ensure the personal data you are processing is:\n\nadequate sufficient to properly fulfil your stated purpose;\nrelevant has a rational link to that purpose; and\nlimited to what is necessary - you do not hold more than you need for that purpose.\n\n\n\nAccuracy\n\nYou should take all reasonable steps to ensure the personal data you hold is not incorrect or misleading as to any matter of fact.\nYou may need to keep the personal data updated, although this will depend on what you are using it for.\nIf you discover that personal data is incorrect or misleading, you must take reasonable steps to correct or erase it as soon as possible.\nYou must carefully consider any challenges to the accuracy of personal data.\n\n\n\nStorage limitation\n\nYou must not keep personal data for longer than you need it.\nYou need to think about and be able to justify how long you keep personal data. This will depend on your purposes for holding the data.\nYou need a policy setting standard retention periods wherever possible, to comply with documentation requirements.\nYou should also periodically review the data you hold, and erase or anonymise it when you no longer need it.\nYou must carefully consider any challenges to your retention of data.\nIndividuals have a right to erasure if you no longer need the data.\nYou can keep personal data for longer if you are only keeping it for public interest archiving, scientific or historical research, or statistical purposes.\n\n\n\nIntegrity and confidentiality (security)\n\nYou must ensure that you have appropriate security measures in place to protect the personal data you hold.\n\n\n\nAccountability\n\nThe accountability principle requires you to take responsibility for what you do with personal data and how you comply with the other principles.\nYou must have appropriate measures and records in place to be able to demonstrate your compliance."
  },
  {
    "objectID": "week6.html#ethical-standards",
    "href": "week6.html#ethical-standards",
    "title": "6  Clean and ethical data",
    "section": "6.4 Ethical Standards",
    "text": "6.4 Ethical Standards\n\nThe IFoA and RSS Guide for Ethical Data Science\nThe Institute and Faculty of Actuaries and the Royal Statistical Society have jointly produced a Guide for Ethical Data Science (IFoA and RSS 2019) which is quoted below.\nThe Guide has five themes:\n\nSeek to enhance the value of data science for society\nAvoid harm\nApply and maintain professional competence\nSeek to preserve or increase trustworthiness\nMaintain accountability and oversight\n\n\nSeek to enhance the value of data science for society\n\nAs the impact that data science can have on society could be significant, an important ethical consideration is what the potential implications could be on society as a whole.\n\n\nA common theme within ethical frameworks discussing data science and AI is for practitioners to attempt to seek outcomes within their work which support the improvement of public wellbeing. This could involve practitioners seeking to share the benefits of data science and balancing this with the wellbeing of potentially affected individuals.\n\n\n\nAvoid harm\n\nData science has the potential to cause harm and this ethical consideration therefore focuses on how practitioners can avoid this by working in a manner that respects the privacy, equality and autonomy of individuals and groups, and speaking up about potential harm or ethical violations.\n\n\nPractitioners may be subject to legal and regulatory obligations in relation to the privacy of individuals, relevant to the jurisdiction in which they are working, as well as regulatory obligations to speak up about harm or violations of legal requirements.\n\n\nThis can also be applied to work relating to businesses, animals or the environment, with consideration of commercial rights, animal welfare and the protection of environmental resources.\n\nThe recent EU paper on profiling and automatic decision making is of interest in this context.\n\n\nApply and maintain professional competence\n\nThis ethical principle expects data science practitioners to apply best practice and comply with all relevant legal and regulatory requirements, as well as applicable professional body codes.\n\n\nProfessional competence involves fully understanding the sources of error and bias in data, using ‘clean’ data (eg edited for missing, inconsistent or erroneous values), and supporting work with robust statistical and algorithmic methods that are appropriate to the question being asked.\n\n\nPractitioners can also thoroughly assess and balance the benefits of the work versus the risks posed by it, and keep models under regular review.\n\n\n\nSeek to preserve or increase trustworthiness\n\nThe public’s trust and confidence in the work of data scientists can be affected by the way ethical principles are applied. Practitioners can help to increase the trustworthiness of their work by considering ethical principles throughout all stages of a project.\n\n\nThis is another reoccurring theme that encourages practitioners to be transparent and honest when communicating about the way data is used. Transparency can include fully explaining how algorithms are being used, if and why any decisions have been delegated, and being open about the risks and biases.\n\n\nEngaging widely with a diverse range of stakeholders and considering public perceptions both from the outset, and throughout projects, can help to build trustworthiness and ensure all potential biases are understood.\n\n\n\nMaintain accountability and oversight\n\nAnother key issue in data ethics around automation and AI is the question of how practitioners maintain human accountability and oversight within their work.\n\n\nBeing accountable can include being mindful of how and when to delegate any decision making to systems, and having governance in place to ensure systems deliver the intended objectives.\n\n\nWhen deciding to delegate any decision making, it would be useful to fully understand and explain the potential implications of doing so, as the work could lead to introducing advanced AI systems which do not have adequate governance. Practitioners should note that delegating any decisions to these systems does not remove any of their individual responsibilities.\n\n\n\n\nOther ethics frameworks\nYou should research any other ethical frameworks relevant to the jurisdiction and professional area you’re working in.\nFor example, the UK Government has published a Data Ethics Framework.\nAnd here is a link to the IEEE’s work on the ethics of autonomous and intelligent systems.\n\n\n\n\nIFoA, and RSS. 2019. “A Guide to Ethical Data Science.” Institute; Faculty of Actuaries; Royal Statistical Society. https://www.actuaries.org.uk/system/files/field/document/An%20Ethical%20Charter%20for%20Date%20Science%20WEB%20FINAL.PDF.\n\n\nJonge, E. de, and M. van der Loo. 2013. “An Introduction to Data Cleaning with r.” Discussion Paper / Statistics Netherlands. Statistics Netherlands. https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.\n\n\nLoo, Mark P. J. van der, and Edwin de Jonge. 2021. “Data Validation Infrastructure for r.” Journal of Statistical Software 97 (10): 1–31. https://doi.org/10.18637/jss.v097.i10."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "7  Some other data structures",
    "section": "",
    "text": "This week we’ll be covering different data structures:\n\nXML\nJSON\nunstructured (or semi-structured) text"
  },
  {
    "objectID": "week7.html#xml",
    "href": "week7.html#xml",
    "title": "7  Some other data structures",
    "section": "7.2 XML",
    "text": "7.2 XML\nExtensible Markup Language (XML) is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.\nXML can serve as the basis for defining markup languages for particular domains. For example XBRL (Extensible Business Reporting Language), KML (Keyhole Markup Language for geographic information), BeerXML (you guessed it).\nHere is a simple example containing some data about my pets.\n<pets>\n  <pet id = '001' species = 'dog'>\n    <tag>Rover</tag>\n    <colour>black</colour>\n  </pet>\n  <pet id = '002' species = 'cat'>\n    <tag>Tiddles</tag>\n    <colour>ginger</colour>\n  </pet>\n  <pet id = '003' species = 'dog'>\n    <tag>Fido</tag>\n    <colour>brownish</colour>\n  </pet>\n</pets>\nEven if you knew nothing about XML before, you can work out what is going on.\n\nPackage xml2\nThe package xml2 gives you tools to read an XML file and extract the data.\nhttps://blog.rstudio.com/2015/04/21/xml2/\nNormally you would read the data from an external file, but for a very small example I’ve saved the data above as a string called my_pets.\n\n\nNavigating the tree\nIn XML everything is arranged in a tree structure, and each element is called a node.\nYou are familiar with a tree structure - just think of the way the files are organised on your computer: folders sit inside folders and each folder can contain other folders and files.\nIn a similar way, nodes sit inside nodes and each node can contain other nodes. The first node, that contains everything else, is the root node, and a node that doesn’t contain any other nodes is called a leaf node.\nIn our example … is the root node, and the text strings “Fido”, “Brownish” etc are leaf nodes.\nNodes can also contain attributes. For example the … nodes have “id” and “species” attributes.\nEvery node, except the root, has exactly one parent, and nodes can have children and siblings (nodes with the same parent).\nWe can use these concepts to navigate the tree.\n\nxpets <- read_xml(my_pets)\nxml_name(xpets)  # The name of the root node\n\n[1] \"pets\"\n\nxml_child(xpets) # Finds the first child of the root (Rover)\n\n{xml_node}\n<pet id=\"001\" species=\"dog\">\n[1] <tag>Rover</tag>\n[2] <colour>black</colour>\n\nxml_children(xpets) # Finds all the children of the root (Rover, Fido & Tibbles)\n\n{xml_nodeset (3)}\n[1] <pet id=\"001\" species=\"dog\">\\n  <tag>Rover</tag>\\n  <colour>black</colour ...\n[2] <pet id=\"002\" species=\"cat\">\\n  <tag>Tiddles</tag>\\n  <colour>ginger</col ...\n[3] <pet id=\"003\" species=\"dog\">\\n  <tag>Fido</tag>\\n  <colour>brownish</colo ...\n\nxml_children(xpets) |> xml_name() # The name of each child\n\n[1] \"pet\" \"pet\" \"pet\"\n\nxml_child(xpets) |> xml_siblings() # the siblings of Rover\n\n{xml_nodeset (2)}\n[1] <pet id=\"002\" species=\"cat\">\\n  <tag>Tiddles</tag>\\n  <colour>ginger</col ...\n[2] <pet id=\"003\" species=\"dog\">\\n  <tag>Fido</tag>\\n  <colour>brownish</colo ...\n\nxml_child(xpets) |> xml_parent() # The parent of Rover\n\n{xml_node}\n<pets>\n[1] <pet id=\"001\" species=\"dog\">\\n  <tag>Rover</tag>\\n  <colour>black</colour ...\n[2] <pet id=\"002\" species=\"cat\">\\n  <tag>Tiddles</tag>\\n  <colour>ginger</col ...\n[3] <pet id=\"003\" species=\"dog\">\\n  <tag>Fido</tag>\\n  <colour>brownish</colo ...\n\nxml_child(xpets) |> xml_child() # <tag> and <colour> are children of Rover\n\n{xml_node}\n<tag>\n\n\n\n\nSearching\nWe can navigate and search through the tree with more precision using XPath.\n\nxml_find_first(xpets, '//pet[@species=\"cat\"]') # Find the first cat\n\n{xml_node}\n<pet id=\"002\" species=\"cat\">\n[1] <tag>Tiddles</tag>\n[2] <colour>ginger</colour>\n\n\n\nxpets |> \n      xml_find_all(\"//pet[@species='dog']\") |> # List the dogs' names\n      xml_find_all(\".//tag\") |>                # Note the important .\n      xml_text()\n\n[1] \"Rover\" \"Fido\" \n\n\n\nxpets |>\n  xml_find_all(\".//pet[@species='dog']\") |>\n  xml_attr(\"id\")\n\n[1] \"001\" \"003\"\n\n\n\n\nCreate a data frame\nBy extracting each quantity we want separately we can put together a data frame.\n\npet_id <- \nxpets |>\n  xml_find_all(\".//pet\") |>\n  xml_attr(\"id\")\n\n\npet_species <- \nxpets |>\n  xml_find_all(\".//pet\") |>\n  xml_attr(\"species\")\n\n\npet_name <- \n  xpets |> \n  xml_find_all(\".//pet/tag\") |> \n  xml_text()\n\n\npet_colour <- \n  xpets |> \n  xml_find_all(\".//pet/colour\") |> \n  xml_text()\n\n\ndfpets <- \n  data.frame(ID = pet_id,\n             Name = pet_name,\n             Species = pet_species,\n             Colour = pet_colour)\ndfpets\n\n   ID    Name Species   Colour\n1 001   Rover     dog    black\n2 002 Tiddles     cat   ginger\n3 003    Fido     dog brownish"
  },
  {
    "objectID": "week7.html#json",
    "href": "week7.html#json",
    "title": "7  Some other data structures",
    "section": "7.3 JSON",
    "text": "7.3 JSON\nJSON is a syntax for storing and exchanging data. It’s lightweight, human readable, language-independent and very widely used. Most programming languages can process JSON.\nTo see what JSON looks like we’ll use the jsonlite package to convert our pets data from a data frame to JSON.\n\nlibrary(jsonlite)\njpets <- toJSON(dfpets)\njpets\n\n[{\"ID\":\"001\",\"Name\":\"Rover\",\"Species\":\"dog\",\"Colour\":\"black\"},{\"ID\":\"002\",\"Name\":\"Tiddles\",\"Species\":\"cat\",\"Colour\":\"ginger\"},{\"ID\":\"003\",\"Name\":\"Fido\",\"Species\":\"dog\",\"Colour\":\"brownish\"}] \n\n\nJSON stands for JavaScript Object Notation. The text inside each set of curly brackets represents a JavaScript object - but it is just text and can be taken to JSON is independent of JavaScript. You can think of each object as being like a card in an old-fashioned card index system.\n\nWe can make it easier to read:\n\nprettify((jpets))\n\n[\n    {\n        \"ID\": \"001\",\n        \"Name\": \"Rover\",\n        \"Species\": \"dog\",\n        \"Colour\": \"black\"\n    },\n    {\n        \"ID\": \"002\",\n        \"Name\": \"Tiddles\",\n        \"Species\": \"cat\",\n        \"Colour\": \"ginger\"\n    },\n    {\n        \"ID\": \"003\",\n        \"Name\": \"Fido\",\n        \"Species\": \"dog\",\n        \"Colour\": \"brownish\"\n    }\n]\n \n\n\njsonlitehas a function to convert to a data frame.\n\nfromJSON(jpets)\n\n   ID    Name Species   Colour\n1 001   Rover     dog    black\n2 002 Tiddles     cat   ginger\n3 003    Fido     dog brownish"
  },
  {
    "objectID": "week7.html#text-processing",
    "href": "week7.html#text-processing",
    "title": "7  Some other data structures",
    "section": "7.4 Text processing",
    "text": "7.4 Text processing\nText processing (a branch of Natural Language Processing, or NLP) is a big topic and we can only scratch the surface in this module.\nIn Wednesday’s class we will look at an example of sentiment analysis.\nWe’ll use a list of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011.(Nielsen 2011) but I have removed profanities from the list because I used it for a presentation in a school.\nWe’ll also use some classic texts downloaded from Project Gutenburg.\nPlease see the Further Reading for more information on text processing (and you’ll probably want to refresh your regex skills)."
  },
  {
    "objectID": "week7.html#reading",
    "href": "week7.html#reading",
    "title": "7  Some other data structures",
    "section": "7.5 Reading",
    "text": "7.5 Reading\n\nXML\n\nWhat is XML (Just the first bit)\nParse and process XML (and HTML) with xml2\n\nJSON\n\nIntro to JSON"
  },
  {
    "objectID": "week7.html#further-reading",
    "href": "week7.html#further-reading",
    "title": "7  Some other data structures",
    "section": "7.6 Further reading",
    "text": "7.6 Further reading\nText Mining with R: a tidy approach (Julia and David 2017) is an excellent introduction, compatible with the methods used in this course."
  },
  {
    "objectID": "week7.html#check-your-understanding",
    "href": "week7.html#check-your-understanding",
    "title": "7  Some other data structures",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\n\n\n\n\nConvert the starwars data to JSON and examine the structure.\n\n\n\n\n\n\njstarwars <- toJSON(starwars, pretty = TRUE)\n\n\n\n\n\n\n\n\n\n\nTry converting back to a data frame and compare with the original.\n\n\n\n\n\n\ndfstarwars <- fromJSON(jstarwars)\n\nidentical(starwars, dfstarwars)\n\n[1] FALSE\n\n\nidentical() returns FALSE. Can you work out why?\n\n\n\n\n\n\n\nJulia, PhD Silge, and PhD Robinson David. 2017. Text Mining with r: A Tidy Approach. O’Reilly Media. https://www.tidytextmining.com/index.html.\n\n\nNielsen, F. Å. 2011. “AFINN.” Richard Petersens Plads, Building 321, DK-2800 Kgs. Lyngby: Informatics; Mathematical Modelling, Technical University of Denmark. http://www2.compute.dtu.dk/pubdb/pubs/6010-full.html."
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "8  SQL",
    "section": "",
    "text": "This week we’ll be looking at the database language SQL.\nYou can’t really be a data scientist without knowing something about SQL - it’s the language used to interact with most of the databases (small and very large) in the world.\nIn the task for this week you will use the RSQLite(Müller et al. 2023) package to create a local database (using SQLite) which you will then use as a source to answer various queries\nStart by skimming through the first few sections of the w3schools tutorial listed in the reading.\nHave a quick look at the documentation for RSQLite (also referenced in the reading).\nThen, open the file QueryDemos.Rmd and get working!\nFor interest, there’s information about connecting to external databases here: https://db.rstudio.com/ ."
  },
  {
    "objectID": "week8.html#other-types-of-database.",
    "href": "week8.html#other-types-of-database.",
    "title": "8  SQL",
    "section": "8.2 Other types of database.",
    "text": "8.2 Other types of database.\nSQL is designed to work with so-called relational databases. These are the most common sort of database, where the data is store in tables, much like data frames in concept. There is both strong theoretical backing and decades of technical development to support relational databases but there are other possibilities and, as always, the best solution depends on the task and the context.\nAlternatives to relational databases are often called NoSQL (where the “No” stands for “Not only”. The topic is a bit beyond the scope of this module but it’s an interesting and important one, so I thought I’d give you some brief pointers and I encourage you to do your own reading if you’re interested.\nThe main types of NoSQL data bases are:\n\nkey-value\n\na key-value database (or “store”) is a set of unique identifiers (the keys) each of which has an associated value. You’er probably familiar with this concept, for example we saw it when we were looking at JSON.\n\ncolumn-family (or column-oriented)\n\ncolumn-family databases are optimised for aggregating functions (down columns) like sum, maximum, etc. So they can be useful for applications where analytics are important.\n\ndocument databases\n\na document database is a type of key-value database where the “value” is a document with its own internal structure that can be queried. Probably the best known supplier is MongoDB\n\ngraph databases\n\nin a graph database data is stored in the form of a graph (the mathematical type of graph consisting of nodes and edges, not a bar chart!). The edges, or links, between nodes carry semantic information, for example (Nigel) -- teaches -- (statistics). Here, (Nigel) and (statistics) are nodes and the link between them expresses the relationship."
  },
  {
    "objectID": "week8.html#reading",
    "href": "week8.html#reading",
    "title": "8  SQL",
    "section": "8.3 Reading",
    "text": "8.3 Reading\n\nhttps://www.w3schools.com/sql/sql_intro.asp\nhttps://db.rstudio.com/databases/sqlite/\nhttps://www.datacamp.com/courses/intro-to-sql-for-data-science (if you are interested)\n\n\n\n\n\nMüller, Kirill, Hadley Wickham, David A. James, and Seth Falcon. 2023. RSQLite: ’SQLite’ Interface for r. https://CRAN.R-project.org/package=RSQLite."
  },
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "9  APIs",
    "section": "",
    "text": "This week we’ll be looking at APIs"
  },
  {
    "objectID": "week9.html#definitions",
    "href": "week9.html#definitions",
    "title": "9  APIs",
    "section": "9.2 Definitions",
    "text": "9.2 Definitions\n\nAPI\nAn application programming interface (API) is an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software. (Wikipedia)\n\n\nREST\nRepresentational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. (Wikipedia)\nSpecifically, one of the restful rules is that that you should get data (called a resource) returned when you link to a specific URL.\nThe URL is called a request and what is sent back is called a response.\nYou can use restful APIs to send as well as receive data, but we will only look at how to get data.\nThe API request can be included in a program - so you don’t need a user to click on a download link.\nAnother piece of jargon is endpoint. This is the base url for the API. This is followed by a path that points to the exact resource.\nFinally we can have query parameters. These always begin with a ? and look like:\n?query1=param1&query2=param2\nwhere the & separates two query/parameter pairs.\nLet’s have an example."
  },
  {
    "objectID": "week9.html#example",
    "href": "week9.html#example",
    "title": "9  APIs",
    "section": "9.3 Example",
    "text": "9.3 Example\nThe endpoint for Github is: https://api.github.com\nThe path to a specific user’s repos is /users/<username>/repos.\nTry copying https://api.github.com/users/vivait/repos into your browser…\nyou should see information returned in JSON.\nBut we want to access the data in a program, not via a browser.\nThe package httr provides tools for HTTP, including the verb GET:\n\nlibrary(dplyr)\nlibrary(jsonlite)\nlibrary(httr)\n\n\ngithub_api <- function(path) {\n  url <- modify_url(\"https://api.github.com\", path = path)\n  GET(url)\n}\n\nresp <- github_api(\"/users/actuarial-science/repos\")\n\nWe can use jsonlite to parse the content of the response into a useful R object.\n\nrepos <- fromJSON(content(resp, \"text\"))\n\nWe can add some parameters to our query\n\nresp <- github_api(\"/users/vivait/repos?sort=updated&per_page=100\")\nrepos <- fromJSON(content(resp, \"text\"))\n\nIn fact, if we know the request will return JSON, we can parse it directly with jsonlite. (Not advised in a program.)\nFor example, the Github documentation says You can issue a GET request to the root endpoint to get all the endpoint categories that the REST API v3 supports:\n\nhead(fromJSON(\"https://api.github.com\"), 10)\n\n$current_user_url\n[1] \"https://api.github.com/user\"\n\n$current_user_authorizations_html_url\n[1] \"https://github.com/settings/connections/applications{/client_id}\"\n\n$authorizations_url\n[1] \"https://api.github.com/authorizations\"\n\n$code_search_url\n[1] \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\"\n\n$commit_search_url\n[1] \"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\"\n\n$emails_url\n[1] \"https://api.github.com/user/emails\"\n\n$emojis_url\n[1] \"https://api.github.com/emojis\"\n\n$events_url\n[1] \"https://api.github.com/events\"\n\n$feeds_url\n[1] \"https://api.github.com/feeds\"\n\n$followers_url\n[1] \"https://api.github.com/user/followers\""
  },
  {
    "objectID": "week9.html#twitter-example",
    "href": "week9.html#twitter-example",
    "title": "9  APIs",
    "section": "9.4 Twitter example",
    "text": "9.4 Twitter example\nNOTE the Twitter (X) API examples below, no longer work (thanks Elon)\nThey will be replaced soon.\n\n\n\nThis code demonstrates how to use the rtweet package.\nFor more detail, see https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html.\nFirst you’ll need to set up a developer account with Twitter and get the access keys you need by creating a new app.\nFollow the instructions at: https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html.\n\n# library(rtweet)\n# ## authenticate - insert your app name and keys below\n# token <- create_token(\n#   app = \"R camlad\",\n#   consumer_key = api_key,\n#   consumer_secret = api_secret_key,\n#   access_token = access_token,\n#   access_secret = access_token_secret)\n\n\nFollowing a hashtag\nWe can search for tweets including a particular hashtag.\n\n## search for tweets using the Cardano hashtag\n# rt <- search_tweets(\"#Cardano\", n = 100, include_rts = FALSE)\n# \n# ## preview tweets data\n# rt |> select(id, text)\n\n\n\nTrending in Leicester\n\n# trnds <- get_trends(\"Leicester\")\n# trnds |> \n#   select(trend, tweet_volume) |> \n#   arrange(desc(tweet_volume))\n\n\n\nGet a particular user’s timeline\n\nlibrary(stringr)\n# tmls <- get_timeline(\"leicspolice\", n = 100)\n# \n# tmls |> \n#   select(created_at, text) |> \n#   filter(str_detect(text, 'Traffic'))"
  },
  {
    "objectID": "week9.html#accessing-uk-census-and-other-data",
    "href": "week9.html#accessing-uk-census-and-other-data",
    "title": "9  APIs",
    "section": "9.5 Accessing UK census (and other) data",
    "text": "9.5 Accessing UK census (and other) data\nOur final example demonstrates the NOMIS API, which can be accessed through the nomisr(Odell 2018) package.\n\nA quick demonstration of using nomisr to extract data from the Nomis API\nThis example is based on the nomisr introduction vignette\n\nlibrary(nomisr)\n\nFirst, we can download information on what data is available.\n\ndata_info <- nomis_data_info()\n#head(data_info)\nglimpse(data_info)\n\nRows: 1,605\nColumns: 14\n$ agencyid                             <chr> \"NOMIS\", \"NOMIS\", \"NOMIS\", \"NOMIS…\n$ id                                   <chr> \"NM_1_1\", \"NM_2_1\", \"NM_4_1\", \"NM…\n$ uri                                  <chr> \"Nm-1d1\", \"Nm-2d1\", \"Nm-4d1\", \"Nm…\n$ version                              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ annotations.annotation               <list> [<data.frame[10 x 2]>], [<data.f…\n$ components.attribute                 <list> [<data.frame[7 x 4]>], [<data.fr…\n$ components.dimension                 <list> [<data.frame[5 x 3]>], [<data.fr…\n$ components.primarymeasure.conceptref <chr> \"OBS_VALUE\", \"OBS_VALUE\", \"OBS_VA…\n$ components.timedimension.codelist    <chr> \"CL_1_1_TIME\", \"CL_2_1_TIME\", \"CL…\n$ components.timedimension.conceptref  <chr> \"TIME\", \"TIME\", \"TIME\", \"TIME\", \"…\n$ description.value                    <chr> \"Records the number of people cla…\n$ description.lang                     <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en…\n$ name.value                           <chr> \"Jobseeker's Allowance with rates…\n$ name.lang                            <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en…\n\n\nThere’s a lot here (data_info has 1605 rows). To dig deeper we can search the column description.value or name.value for key words.\n\npop_data_info <- \n  data_info |> \n  filter(str_detect(name.value, \"(?i)population\")) |> \n  select(id, name.value)\n\n#pop_data_info |> head()\nglimpse(pop_data_info)\n\nRows: 110\nColumns: 2\n$ id         <chr> \"NM_17_1\", \"NM_17_5\", \"NM_31_1\", \"NM_100_1\", \"NM_136_1\", \"N…\n$ name.value <chr> \"annual population survey\", \"annual population survey (vari…\n\n\nSuppose we wanted population data for Leicester. It looks like “NM_31_1” might be worth investigating, so we can dig down deeper.\nThe data or is categorised first by “concept” (Read the docs at nomis if you want more details.)\n\nid = \"NM_31_1\"\nnomis_get_metadata(id)\n\n# A tibble: 6 × 3\n  codelist          conceptref isfrequencydimension\n  <chr>             <chr>      <chr>               \n1 CL_31_1_GEOGRAPHY GEOGRAPHY  false               \n2 CL_31_1_SEX       SEX        false               \n3 CL_31_1_AGE       AGE        false               \n4 CL_31_1_MEASURES  MEASURES   false               \n5 CL_31_1_FREQ      FREQ       true                \n6 CL_31_1_TIME      TIME       false               \n\n\nGEOGRAPHY looks relevant, so we explore what “types” are available.\n\nnomis_get_metadata(id, \"GEOGRAPHY\", type = \"type\")\n\n# A tibble: 26 × 3\n   id      label.en                                               description.en\n   <chr>   <chr>                                                  <chr>         \n 1 TYPE83  jobcentre plus group as of April 2019                  jobcentre plu…\n 2 TYPE84  jobcentre plus district as of April 2019               jobcentre plu…\n 3 TYPE342 english index of multiple deprivation 2010 - deciles   english index…\n 4 TYPE347 scottish index of multiple deprivation 2009 - deciles  scottish inde…\n 5 TYPE349 welsh index of multiple deprivation 2008 - deciles     welsh index o…\n 6 TYPE431 local authorities: county / unitary (as of April 2021) local authori…\n 7 TYPE432 local authorities: district / unitary (as of April 20… local authori…\n 8 TYPE433 local authorities: county / unitary (as of April 2019) local authori…\n 9 TYPE434 local authorities: district / unitary (as of April 20… local authori…\n10 TYPE442 combined authorities                                   combined auth…\n# ℹ 16 more rows\n\n\nFinally, we can choose a particular type and investigate it.\n\nid |> \n  nomis_get_metadata(\"GEOGRAPHY\", type = \"TYPE446\") |> \n  filter(str_detect(label.en, \"Leicester\"))\n\n# A tibble: 2 × 4\n  id         parentCode label.en       description.en\n  <chr>      <chr>      <chr>          <chr>         \n1 1870659636 2013265924 Leicester      Leicester     \n2 1870659640 2013265924 Leicestershire Leicestershire\n\n\nLooks like we’ve found what we want!\n\nleics_pop <- \n  nomis_get_data(id = id, time = \"latest\",\n                 geography = c(\"1870659636\", \"1870659640\"))\n\nleics_pop |> \n  select(DATE, GEOGRAPHY_NAME, SEX_NAME, AGE_NAME, MEASURES_NAME, OBS_VALUE) |> \n  head(10)\n\n# A tibble: 10 × 6\n    DATE GEOGRAPHY_NAME SEX_NAME AGE_NAME           MEASURES_NAME OBS_VALUE\n   <dbl> <chr>          <chr>    <chr>              <chr>             <dbl>\n 1  2021 Leicester      Male     All ages           Value                NA\n 2  2021 Leicester      Male     All ages           Percent              NA\n 3  2021 Leicester      Male     Aged under 1 year  Value                NA\n 4  2021 Leicester      Male     Aged under 1 year  Percent              NA\n 5  2021 Leicester      Male     Aged 1 - 4 years   Value                NA\n 6  2021 Leicester      Male     Aged 1 - 4 years   Percent              NA\n 7  2021 Leicester      Male     Aged 5 - 9 years   Value                NA\n 8  2021 Leicester      Male     Aged 5 - 9 years   Percent              NA\n 9  2021 Leicester      Male     Aged 10 - 14 years Value                NA\n10  2021 Leicester      Male     Aged 10 - 14 years Percent              NA"
  },
  {
    "objectID": "week9.html#homework",
    "href": "week9.html#homework",
    "title": "9  APIs",
    "section": "9.6 Homework",
    "text": "9.6 Homework\nInstall the package randNames and, using the instructions in the package documentation register for a free API key at randomapi.com.\nWrite a programme to download random data for 400 imaginary users. What is the distribution of genders and country of origin in this data.\n\nOptional Christmas Bonus question\nRegister an account at Advent of Code. For the 2020 competition solve Question 2. (The key to solving this elegantly is reading the data in and wrangling it into the best format to solve the problem.)\n\n\n\n\nOdell, Evan. 2018. “nomisr: Access Nomis UK Labour Market Data with r.” The Journal of Open Source Software 3 (27): 859. https://doi.org/10.21105/joss.00859."
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "10  Final reports",
    "section": "",
    "text": "In Week 10 you will be submitting your final reports."
  },
  {
    "objectID": "week10.html#citing-r-and-packages",
    "href": "week10.html#citing-r-and-packages",
    "title": "10  Final reports",
    "section": "10.2 Citing R and packages",
    "text": "10.2 Citing R and packages\nThe underlying programme is clearly a massive piece of work and you should give its authors credit by citing them in any work you do using R.\nFor example you might write:\n\nStatistical analysis was done using R 4.1.1 (R Core Team 2022).\n\nR makes it easy to generate the right reference to use because there’s a built-in function to do it.\n\ncitation()\n\n\nTo cite R in publications use:\n\n  R Core Team (2022). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nYou should also cite the main packages you’ve used. For example:\n\nData wrangling was carried out with dplyr (Wickham et al. 2023) and other packages from the tidyverse (Wickham et al. 2019); graphs were plotted using ggplot2 (Wickham 2016).\n\nAgain, you can use citation to generate the correct references. For example:\n\ncitation('dplyr')\n\n\nTo cite package 'dplyr' in publications use:\n\n  Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A\n  Grammar of Data Manipulation_. R package version 1.1.3,\n  <https://CRAN.R-project.org/package=dplyr>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {dplyr: A Grammar of Data Manipulation},\n    author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},\n    year = {2023},\n    note = {R package version 1.1.3},\n    url = {https://CRAN.R-project.org/package=dplyr},\n  }\n\n\nIf you are wondering, BibTeX is a file format (and software) used to describe lists of references, often for use within LaTeX documents. The rmarkdown package (Xie, Allaire, and Grolemund 2018) provides methods to work with BibTeX references.\n\nWhat to cite?\nYou should always cite R itself, but there is an element of judgement in deciding which individual packages to cite. For this module, you won’t lose marks as long as you have made a reasonable effort.\nTo make your work fully reproducible it is best practice to list all the packages you’ve used (e.g. in an appendix). You can generate the necessary information with the function sessionInfo(), but this is not necessary for FDS coursework.\n\n\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chang, Winston. 2020. R Graphics Cookbook. O’Reilly Media. https://r-graphics.org/.\n\n\nFriedl, Jeffrey E. F. 2006. Mastering Regular Expressions. 3rd\ned.. Sebastapol, Calif.: O’Reilly.\n\n\nIFoA, and RSS. 2019. “A Guide to Ethical Data Science.”\nInstitute; Faculty of Actuaries; Royal Statistical Society. https://www.actuaries.org.uk/system/files/field/document/An%20Ethical%20Charter%20for%20Date%20Science%20WEB%20FINAL.PDF.\n\n\nJonge, E. de, and M. van der Loo. 2013. “An Introduction to Data\nCleaning with r.” Discussion Paper / Statistics Netherlands.\nStatistics Netherlands. https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.\n\n\nJulia, PhD Silge, and PhD Robinson David. 2017. Text Mining with r:\nA Tidy Approach. O’Reilly Media. https://www.tidytextmining.com/index.html.\n\n\nLoo, Mark P. J. van der, and Edwin de Jonge. 2021. “Data\nValidation Infrastructure for r.” Journal of Statistical\nSoftware 97 (10): 1–31. https://doi.org/10.18637/jss.v097.i10.\n\n\nMüller, Kirill, Hadley Wickham, David A. James, and Seth Falcon. 2023.\nRSQLite: ’SQLite’ Interface for r. https://CRAN.R-project.org/package=RSQLite.\n\n\nNielsen, F. Å. 2011. “AFINN.” Richard Petersens Plads,\nBuilding 321, DK-2800 Kgs. Lyngby: Informatics;\nMathematical Modelling, Technical University of Denmark. http://www2.compute.dtu.dk/pubdb/pubs/6010-full.html.\n\n\nOdell, Evan. 2018. “nomisr: Access\nNomis UK Labour Market Data with r.” The Journal of Open\nSource Software 3 (27): 859. https://doi.org/10.21105/joss.00859.\n\n\nPeng, Roger D. 2020. R Programming for Data Science.\nMorrisville: Lulu.com. https://bookdown.org/rdpeng/rprogdatascience/.\n\n\nPeng, Roger D. 2019. Report Writing for Data Science in r.\nBritish Columbia, Canada: Leanpub. https://leanpub.com/reportwriting.\n\n\nPosit team. 2023. RStudio: Integrated Development Environment for\nr. Boston, MA: Posit Software, PBC. http://www.posit.co/.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRobin Lovelace, Jannes Muenchow, Jakub Nowosad. 2020. Geocomputation\nwith r. CRC Press. https://geocompr.robinlovelace.net/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\nhttp://r4ds.had.co.nz/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown:\nThe Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown."
  }
]